<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Devin</title>
  
  <subtitle>Devin</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="dengcongbao.github.io/"/>
  <updated>2018-02-28T05:11:19.103Z</updated>
  <id>dengcongbao.github.io/</id>
  
  <author>
    <name>Devin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Docker-Notes</title>
    <link href="dengcongbao.github.io/2018/02/02/Docker-Notes/"/>
    <id>dengcongbao.github.io/2018/02/02/Docker-Notes/</id>
    <published>2018-02-02T05:03:49.000Z</published>
    <updated>2018-02-28T05:11:19.103Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker三大核心"><a href="#Docker三大核心" class="headerlink" title="Docker三大核心"></a>Docker三大核心</h1><h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2><h3 id="获取镜像"><a href="#获取镜像" class="headerlink" title="获取镜像"></a>获取镜像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull</span><br></pre></td></tr></table></figure><h3 id="镜像加速："><a href="#镜像加速：" class="headerlink" title="镜像加速："></a>镜像加速：</h3><ol><li>登录阿里云 进入控制台。</li><li>点击“产品与服务”找到“容器服务”。</li><li>点击“镜像”，点击右上角的“镜像仓库控制台”，进入后然后点击“镜像加速器” 。</li><li>点击镜像加速器 按照文档配置即可。</li></ol><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><h4 id="创建容器："><a href="#创建容器：" class="headerlink" title="创建容器："></a>创建容器：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 16379:6379 --name redis redis:3.0</span><br><span class="line">docker run --name db -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql</span><br></pre></td></tr></table></figure><h4 id="启动容器："><a href="#启动容器：" class="headerlink" title="启动容器："></a>启动容器：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker start redis</span><br><span class="line">docker start db</span><br></pre></td></tr></table></figure><h4 id="关闭容器："><a href="#关闭容器：" class="headerlink" title="关闭容器："></a>关闭容器：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker stop redis</span><br><span class="line">docker stop db</span><br></pre></td></tr></table></figure><h2 id="仓库"><a href="#仓库" class="headerlink" title="仓库"></a>仓库</h2><p>仓库是集中存放镜像的地方</p><p>目前 Docker 官方维护了一个公共仓库 Docker Hub，其中已经包括了超过               15,000 的镜像。大部分需求，都可以通过在 Docker Hub 中直接下载镜像来实现</p><p>用户无需登录即可通过 docker search 命令来查找官方仓库中的镜像，并利用 docker pull 命令来将它下载到本地。</p><p>阿里云既提供了加速功能，也提供了仓库功能</p>]]></content>
    
    <summary type="html">
    
      Docker是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。
    
    </summary>
    
      <category term="Docker" scheme="dengcongbao.github.io/categories/Docker/"/>
    
    
      <category term="Docker" scheme="dengcongbao.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Centos7安装Docker</title>
    <link href="dengcongbao.github.io/2018/02/02/Centos7%E5%AE%89%E8%A3%85Docker/"/>
    <id>dengcongbao.github.io/2018/02/02/Centos7安装Docker/</id>
    <published>2018-02-02T02:08:29.000Z</published>
    <updated>2018-02-02T02:21:07.069Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>安装docker-io yum install docker-io</p></li><li><p>检查安装情况  docker -h</p></li><li><p>启动 service docker start或者systemctl start docker</p><p>​    </p></li></ol>]]></content>
    
    <summary type="html">
    
      Docker自2013年以来非常火热，无论是从 github 上的代码活跃度，还是Redhat在RHEL6.5中集成对Docker的支持, 就连 Google 的 Compute Engine 也支持 docker 在其之上运行。
    
    </summary>
    
      <category term="Docker" scheme="dengcongbao.github.io/categories/Docker/"/>
    
    
      <category term="Docker" scheme="dengcongbao.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>VirtualBoxNAT模式远程登录Ubuntu虚拟机</title>
    <link href="dengcongbao.github.io/2018/01/31/VirtualBoxNAT%E6%A8%A1%E5%BC%8F%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95Ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    <id>dengcongbao.github.io/2018/01/31/VirtualBoxNAT模式远程登录Ubuntu虚拟机/</id>
    <published>2018-01-31T07:29:20.000Z</published>
    <updated>2018-01-31T09:14:29.909Z</updated>
    
    <content type="html"><![CDATA[<p>配置如下图</p><p><img src="/img/clipboard.png" alt="image"></p><p>ssh客户端配置 ip:127.0.0.1 port:2222 即可访问</p>]]></content>
    
    <summary type="html">
    
      VirtualBoxNAT模式远程登录Ubuntu虚拟机
    
    </summary>
    
      <category term="linux" scheme="dengcongbao.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="dengcongbao.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux下root运行Elasticsearch异常</title>
    <link href="dengcongbao.github.io/2018/01/25/linux%E4%B8%8Broot%E8%BF%90%E8%A1%8CElasticsearch%E5%BC%82%E5%B8%B8/"/>
    <id>dengcongbao.github.io/2018/01/25/linux下root运行Elasticsearch异常/</id>
    <published>2018-01-25T09:30:50.000Z</published>
    <updated>2018-01-25T10:21:44.281Z</updated>
    
    <content type="html"><![CDATA[<p>环境：Centos7  elasticsearch-2.4.5</p><p>异常：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> java.lang.RuntimeException: don<span class="string">'t run elasticsearch as root.</span></span><br><span class="line"><span class="string">        at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:94)</span></span><br><span class="line"><span class="string">        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:160)</span></span><br><span class="line"><span class="string">        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:286)</span></span><br><span class="line"><span class="string">        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:45)</span></span><br><span class="line"><span class="string">Refer to the log for complete error details.</span></span><br></pre></td></tr></table></figure><p>解决方案：</p><p>​              vi elasticsearch</p><p>​              加上 ES_JAVA_OPTS=”-Des.insecure.allow.root=true”</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;环境：Centos7  elasticsearch-2.4.5&lt;/p&gt;
&lt;p&gt;异常：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/sp
      
    
    </summary>
    
      <category term="Elasticsearch" scheme="dengcongbao.github.io/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="dengcongbao.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>破解idea2017.3亲测有效</title>
    <link href="dengcongbao.github.io/2018/01/24/%E7%A0%B4%E8%A7%A3idea2017-3%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88/"/>
    <id>dengcongbao.github.io/2018/01/24/破解idea2017-3亲测有效/</id>
    <published>2018-01-24T02:32:27.000Z</published>
    <updated>2018-01-24T02:45:51.728Z</updated>
    
    <content type="html"><![CDATA[<p>下载这个： <a href="http://idea.lanyus.com/jar/JetbrainsCrack-2.6.10-release-enc.jar" target="_blank" rel="noopener">http://idea.lanyus.com/jar/JetbrainsCrack-2.6.10-release-enc.jar</a>  </p><p> 放在idea的bin目录下</p><p>在idea.exe.vamoptions和idea64.exe.vamoptions中最后一行各自插入这一句</p><p>-javaagent:F:\Program Files\JetBrains\IntelliJ IDEA 2017.3\bin\JetbrainsCrack-2.6.10-release-enc.jar（根据你的安装目录不同需要改变，就是这个jar包的路径）</p><p><a href="http://idea.lanyus.com/" target="_blank" rel="noopener">http://idea.lanyus.com/</a> 获取注册码 填入active code</p><p>接着就重启idea打开查看发现破解到2099年了</p>]]></content>
    
    <summary type="html">
    
      IntelliJ IDEA 破解到2099年
    
    </summary>
    
      <category term="IntelliJ IDEA" scheme="dengcongbao.github.io/categories/IntelliJ-IDEA/"/>
    
    
      <category term="IntelliJ IDEA" scheme="dengcongbao.github.io/tags/IntelliJ-IDEA/"/>
    
  </entry>
  
  <entry>
    <title>HDFS的运行原理</title>
    <link href="dengcongbao.github.io/2018/01/18/HDFS%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/"/>
    <id>dengcongbao.github.io/2018/01/18/HDFS的运行原理/</id>
    <published>2018-01-18T04:58:08.000Z</published>
    <updated>2018-01-18T05:31:19.134Z</updated>
    
    <content type="html"><![CDATA[<p><strong>简介</strong></p><p><strong>HDFS</strong>（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（<a href="http://www.open-open.com/lib/view/open1328763454608.html" target="_blank" rel="noopener">中文</a>，<a href="http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf" target="_blank" rel="noopener">英文</a>）。</p><p><strong>HDFS有很多特点</strong>：</p><p>​    <strong>① </strong>保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。</p><p>​    <strong>② </strong>运行在廉价的机器上。</p><p>​    <strong>③ </strong>适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。</p><p><img src="/img/1.jpg" alt="img"></p><p>如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。</p><p><strong>*NameNode*</strong>：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；</p><p><strong>*SecondaryNameNode*</strong>：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。</p><p><strong>*DataNode</strong>：Slave*节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。</p><p><strong>*热备份*</strong>：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。</p><p><strong>*冷备份*</strong>：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。</p><p><strong>*fsimage*</strong>:元数据镜像文件（文件系统的目录树。）</p><p><strong>*edits*</strong>：元数据的操作日志（针对文件系统做的修改操作记录）</p><p><strong>namenode内存中存储的是=fsimage+edits。</strong></p><p>SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。</p><p><strong>工作原理</strong></p><p><strong>写操作：</strong></p><p><img src="/img/2.jpg" alt="img"></p><p>有一个文件FileA，100M大小。Client将FileA写入到HDFS上。</p><p>HDFS按默认配置。</p><p>HDFS分布在三个机架上Rack1，Rack2，Rack3。</p><p><strong>a.</strong> Client将FileA按64M分块。分成两块，block1和Block2;</p><p><strong>b.</strong> Client向nameNode发送写数据请求，如图蓝色虚线①——&gt;。</p><p><strong>c.</strong> NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②———&gt;。</p><p>​    Block1: host2,host1,host3</p><p>​    Block2: host7,host8,host4</p><p>​    原理：</p><p>​        NameNode具有RackAware机架感知功能，这个可以配置。</p><p>​        若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。</p><p>​        若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。</p><p><strong>d.</strong> client向DataNode发送block1；发送过程是以流式写入。</p><p>​    流式写入过程，</p><p>​       <strong> 1&gt;</strong>将64M的block1按64k的package划分;</p><p>​        <strong>2&gt;</strong>然后将第一个package发送给host2;</p><p>​        <strong>3&gt;</strong>host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；</p><p>​        <strong>4&gt;</strong>host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。</p><p>​        <strong>5&gt;</strong>以此类推，如图红线实线所示，直到将block1发送完毕。</p><p>​        <strong>6&gt;</strong>host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。</p><p>​        <strong>7&gt;</strong>client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线</p><p>​        <strong>8&gt;</strong>发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。</p><p>​        <strong>9&gt;</strong>发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。</p><p>​        <strong>10&gt;</strong>client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。</p><p><strong>分析，</strong>通过写过程，我们可以了解到：</p><p>​    <strong>①</strong>写1T文件，我们需要3T的存储，3T的网络流量贷款。</p><p>​    <strong>②</strong>在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。</p><p>​    <strong>③</strong>挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。</p><p><strong>读操作：</strong></p><p> <img src="/img/3.jpg" alt="img"></p><p>读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。 </p><p>那么，读操作流程为：</p><p><strong>a.</strong> client向namenode发送读请求。</p><p><strong>b.</strong> namenode查看Metadata信息，返回fileA的block的位置。</p><p>​    block1:host2,host1,host3</p><p>​    block2:host7,host8,host4</p><p><strong>c.</strong> block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取；</p><p>上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是：</p><p><strong>优选读取本机架上的数据</strong>。</p>]]></content>
    
    <summary type="html">
    
      HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="dengcongbao.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hbase原理浅谈</title>
    <link href="dengcongbao.github.io/2018/01/18/Hbase%E5%8E%9F%E7%90%86%E6%B5%85%E8%B0%88/"/>
    <id>dengcongbao.github.io/2018/01/18/Hbase原理浅谈/</id>
    <published>2018-01-18T02:10:42.000Z</published>
    <updated>2018-01-18T02:17:00.359Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-hbase原理"><a href="#1-hbase原理" class="headerlink" title="1.  hbase原理"></a><a href="">1.  hbase</a>原理</h1><h2 id="1-1-体系图"><a href="#1-1-体系图" class="headerlink" title="1.1.  体系图"></a><a href="">1.1.  体系图</a></h2><p><img src="/img/hbase-img.png" alt="img"></p><h3 id="1-1-1-写流程"><a href="#1-1-1-写流程" class="headerlink" title="1.1.1.  写流程"></a><a href="">1.1.1.  写流程</a></h3><p>1、  client向hregionserver发送写请求。</p><p>2、  hregionserver将数据写到hlog（write ahead log）。为了数据的持久化和恢复。</p><p>3、  hregionserver将数据写到内存（memstore）</p><p>4、  反馈client写成功。</p><h3 id="1-1-2-数据flush过程"><a href="#1-1-2-数据flush过程" class="headerlink" title="1.1.2.  数据flush过程"></a><a href="">1.1.2.  数据flush</a>过程</h3><p>1、  当memstore数据达到阈值（默认是64M），将数据刷到硬盘，将内存中的数据删除，同时删除Hlog中的历史数据。</p><p>2、  并将数据存储到hdfs中。</p><p>3、  在hlog中做标记点。</p><h3 id="1-1-3-hbase的读流程"><a href="#1-1-3-hbase的读流程" class="headerlink" title="1.1.3.  hbase的读流程"></a><a href="">1.1.3.  hbase</a>的读流程</h3><p>1、  通过zookeeper和-ROOT-.META.表定位hregionserver。</p><p>2、  如果数据在内存，直接将数据读取，返回。</p><p>3、  如果数据在hdfs，从hdfs中读取出来。</p><h3 id="1-1-4-hmaster的职责"><a href="#1-1-4-hmaster的职责" class="headerlink" title="1.1.4.  hmaster的职责"></a><a href="">1.1.4.  hmaster</a>的职责</h3><p>1、管理用户对Table的增、删、改、查操作； </p><p>2、记录region在哪台Hregionserver上</p><p>3、在Region Split后，负责新Region的分配； </p><p>4、新机器加入时，管理HRegionServer的负载均衡，调整Region分布</p><p>5、在HRegion Server停机后，负责失效HRegionServer 上的Regions迁移。</p><h3 id="1-1-5-hregionserver的职责"><a href="#1-1-5-hregionserver的职责" class="headerlink" title="1.1.5.  hregionserver的职责"></a><a href="">1.1.5.  hregionserver</a>的职责</h3><p>HRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBASE中最核心的模块。</p><p>HRegion Server管理了很多table的分区，也就是region。</p><h3 id="1-1-6-client职责"><a href="#1-1-6-client职责" class="headerlink" title="1.1.6.  client职责"></a><a href="">1.1.6.  client</a>职责</h3><p>Client</p><p>HBASE Client使用HBASE的RPC机制与HMaster和RegionServer进行通信</p><p>管理类操作：Client与HMaster进行RPC；</p><p>数据读写类操作：Client与HRegionServer进行RPC。</p>]]></content>
    
    <summary type="html">
    
      Hbase原理浅谈
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hbase" scheme="dengcongbao.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase开发入门指南</title>
    <link href="dengcongbao.github.io/2018/01/18/Hbase%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/"/>
    <id>dengcongbao.github.io/2018/01/18/Hbase开发入门指南/</id>
    <published>2018-01-18T01:56:17.000Z</published>
    <updated>2018-01-18T02:17:04.110Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-hbase数据模型"><a href="#1-hbase数据模型" class="headerlink" title="1.  hbase数据模型"></a><a href="">1.  hbase</a>数据模型</h1><h2 id="1-1-hbase数据模型"><a href="#1-1-hbase数据模型" class="headerlink" title="1.1.  hbase数据模型"></a><a href="">1.1.  hbase</a>数据模型</h2><p><img src="/img/datatype.png" alt="img"></p><h3 id="1-1-1-Row-Key"><a href="#1-1-1-Row-Key" class="headerlink" title="1.1.1.  Row Key"></a><a href="">1.1.1.  Row Key</a></h3><p>与nosql数据库们一样,row key是用来检索记录的主键。访问HBASE table中的行，只有三种方式：</p><p>1.通过单个row key访问</p><p>2.通过row key的range（正则）</p><p>3.全表扫描</p><p>Row key行键 (Row key)可以是任意字符串(最大长度 是 64KB，实际应用中长度一般为10-100bytes)，在HBASE内部，row key保存为字节数组。存储时，数据按照Rowkey的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</p><h3 id="1-1-2-Columns-Family"><a href="#1-1-2-Columns-Family" class="headerlink" title="1.1.2.  Columns Family"></a><a href="">1.1.2.  Columns Family</a></h3><p>列簇：HBASE表中的每个列，都归属于某个列族。列族是表的schema的一部分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如courses:history，courses:math都属于courses 这个列族。</p><h3 id="1-1-3-Cell"><a href="#1-1-3-Cell" class="headerlink" title="1.1.3.  Cell"></a><a href="">1.1.3.  Cell</a></h3><p>由{row key, columnFamily, version} 唯一确定的单元。cell中 的数据是没有类型的，全部是字节码形式存贮。</p><p>关键字：无类型、字节码</p><h3 id="1-1-4-Time-Stamp"><a href="#1-1-4-Time-Stamp" class="headerlink" title="1.1.4.  Time Stamp"></a><a href="">1.1.4.  Time Stamp</a></h3><p>HBASE 中通过rowkey和columns确定的为一个存贮单元称为cell。每个 cell都保存 着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由HBASE(在数据写入时自动 )赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</p><p>为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供 了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段 时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。</p><h1 id="2-hbase命令"><a href="#2-hbase命令" class="headerlink" title="2.  hbase命令"></a><a href="">2.  hbase</a>命令</h1><h2 id="2-1-命令的进退"><a href="#2-1-命令的进退" class="headerlink" title="2.1.  命令的进退"></a><a href="">2.1.  命令的进退</a></h2><p>1、hbase提供了一个shell的终端给用户交互</p><p>#$HBASE_HOME/bin/hbase shell </p><p>2、如果退出执行quit命令</p><p>#$HBASE_HOME/bin/hbase shell</p><p>……</p><p>>quit</p><h2 id="2-2-命令"><a href="#2-2-命令" class="headerlink" title="2.2.  命令"></a><a href="">2.2.  命令</a></h2><table><thead><tr><th><strong>名称</strong></th><th><strong>命令表达式</strong></th></tr></thead><tbody><tr><td><strong>创建表</strong></td><td><strong>create ‘**</strong>表名’, ‘<strong><strong>列族名1’,’</strong></strong>列族名2’,’<strong>**列族名N’</strong></td></tr><tr><td><strong>查看所有表</strong></td><td><strong>list</strong></td></tr><tr><td><strong>描述表</strong></td><td><strong>describe  **</strong>‘表名’**</td></tr><tr><td>判断表存在</td><td>exists  ‘表名’<em>**</em></td></tr><tr><td>判断是否禁用启用表<em>**</em></td><td>is_enabled ‘表名’  is_disabled ‘表名’</td></tr><tr><td><strong>添加记录      </strong></td><td><strong>put   **</strong>‘表名’, <strong><strong>‘rowKey</strong></strong>’, <strong><strong>‘列族 : </strong></strong>列‘  ,  ‘<strong>**值’</strong></td></tr><tr><td><strong>查看记录rowkey**</strong>下的所有数据**</td><td><strong>get   ‘**</strong>表名’ , ‘rowKey’**</td></tr><tr><td><strong>查看表中的记录总数</strong></td><td><strong>count   ‘**</strong>表名’**</td></tr><tr><td><strong>获取某个列族</strong></td><td>get ‘表名’,’rowkey’,’列族’<em>**</em></td></tr><tr><td><strong>获取某个列族的某个列</strong></td><td>get ‘表名’,’rowkey’,’列族：列’</td></tr><tr><td><strong>删除记录</strong></td><td><strong>delete   **</strong>‘表名’ ,<strong><strong>‘行名’ , </strong></strong>‘列族：列’**</td></tr><tr><td><strong>删除整行</strong></td><td><strong>deleteall ‘**</strong>表名’,’rowkey’**</td></tr><tr><td><strong>删除一张表</strong></td><td><strong>先要屏蔽该表，才能对该表进行删除</strong>  <strong>第一步 disable **</strong>‘表名’ <strong><strong>，第二步  drop ‘</strong></strong>表名’**</td></tr><tr><td><strong>清空表</strong></td><td><strong>truncate ‘**</strong>表名’**</td></tr><tr><td><strong>查看所有记录</strong></td><td><strong>scan “**</strong>表名”  **</td></tr><tr><td><strong>查看某个表某个列中所有数据</strong></td><td><strong>scan “**</strong>表名” ,  {COLUMNS=&gt;’<strong><strong>列族名:</strong></strong>列名’}**</td></tr><tr><td><strong>更新记录  </strong></td><td><strong>就是重写一遍，进行覆盖，hbase**</strong>没有修改，都是追加**</td></tr></tbody></table><h1 id="3-hbase依赖zookeeper"><a href="#3-hbase依赖zookeeper" class="headerlink" title="3.  hbase依赖zookeeper"></a><a href="">3.  hbase</a>依赖zookeeper</h1><p>zookeeper的作用：</p><p>1、  保存Hmaster的地址和backup-master地址</p><p>hmaster的作用：</p><p>a)      管理HregionServer</p><p>b)     做增删改查表的节点</p><p>c)      管理HregionServer中的表分配</p><p>2、  保存表-ROOT-的地址</p><p>hbase默认的根表，检索表。</p><p>3、  HRegionServer列表</p><p>表的增删改查数据。</p><p>和hdfs交互，存取数据。</p><h1 id="4-hbase开发"><a href="#4-hbase开发" class="headerlink" title="4.  hbase开发"></a><a href="">4.  hbase</a>开发</h1><h2 id="4-1-配置"><a href="#4-1-配置" class="headerlink" title="4.1.  配置"></a><a href="">4.1.  配置</a></h2><p>HBaseConfiguration</p><p>包：org.apache.hadoop.hbase.HBaseConfiguration</p><p>作用：通过此类可以对HBase进行配置</p><p>用法实例：</p><p>Configuration config =HBaseConfiguration.create();</p><p>说明：HBaseConfiguration.create() 默认会从classpath 中查找hbase-site.xml 中的配置信息，初始化 Configuration。</p><p>使用方法:</p><p>static Configuration config = null;</p><p>static {</p><p>​    config = HBaseConfiguration.create();</p><p>​    config.set(“hbase.zookeeper.quorum”,”slave1,slave2,slave3”);</p><p>​    config.set(“hbase.zookeeper.property.clientPort”,”2181”);</p><p>}</p><h2 id="4-2-表管理类"><a href="#4-2-表管理类" class="headerlink" title="4.2.  表管理类"></a><a href="">4.2.  表管理类</a></h2><p>HBaseAdmin</p><p>包：org.apache.hadoop.hbase.client.HBaseAdmin</p><p>作用：提供接口关系HBase 数据库中的表信息</p><p>用法：</p><p>HBaseAdmin admin = new HBaseAdmin(config);</p><h2 id="4-3-表描述类"><a href="#4-3-表描述类" class="headerlink" title="4.3.  表描述类"></a><a href="">4.3.  表描述类</a></h2><p>HTableDescriptor</p><p>包：org.apache.hadoop.hbase.HTableDescriptor</p><p>作用：HTableDescriptor 类包含了表的名字以及表的列族信息</p><p>​         表的schema（设计）</p><p>用法：</p><p>HTableDescriptor htd =newHTableDescriptor(tablename);</p><p>htd.addFamily(newHColumnDescriptor(“myFamily”));</p><h2 id="4-4-列族的描述类"><a href="#4-4-列族的描述类" class="headerlink" title="4.4.  列族的描述类"></a><a href="">4.4.  列族的描述类</a></h2><p>HColumnDescriptor</p><p>包：org.apache.hadoop.hbase.HColumnDescriptor</p><p>作用：HColumnDescriptor 维护列族的信息</p><p>用法：</p><p>htd.addFamily(newHColumnDescriptor(“myFamily”));</p><h2 id="4-5-创建表的操作"><a href="#4-5-创建表的操作" class="headerlink" title="4.5.  创建表的操作"></a><a href="">4.5.  创建表的操作</a></h2><p>CreateTable（一般我们用shell创建表）</p><p>static Configuration config = null;</p><p>static {</p><p>​    config = HBaseConfiguration.create();</p><p>​    config.set(“hbase.zookeeper.quorum”,”slave1,slave2,slave3”);</p><p>​    config.set(“hbase.zookeeper.property.clientPort”,”2181”);</p><p>}</p><p>HBaseAdmin admin = new HBaseAdmin(config);</p><p>HTableDescriptor desc = newHTableDescriptor(tableName);</p><p>HColumnDescriptor family1 = newHColumnDescriptor(“f1”);</p><p>HColumnDescriptor family2 = new HColumnDescriptor(“f2”);</p><p>desc.addFamily(family1);</p><p>desc.addFamily(family2);</p><p>admin.createTable(desc);</p><h2 id="4-6-删除表"><a href="#4-6-删除表" class="headerlink" title="4.6.  删除表"></a><a href="">4.6.  删除表</a></h2><p>HBaseAdmin admin = new HBaseAdmin(config);</p><p>admin.disableTable(tableName);</p><p>admin.deleteTable(tableName);</p><h2 id="4-7-创建一个表的类"><a href="#4-7-创建一个表的类" class="headerlink" title="4.7.  创建一个表的类"></a><a href="">4.7.  创建一个表的类</a></h2><p>HTable</p><p>包：org.apache.hadoop.hbase.client.HTable</p><p>作用：HTable 和 HBase 的表通信</p><p>用法：</p><p>// 普通获取表</p><p> HTable table = newHTable(config,Bytes.toBytes(tablename);</p><p>// 通过连接池获取表</p><p>HTablePool pool = new HTablePool(config,1000);</p><p>HTableInterface table =pool.getTable(tableName);</p><h2 id="4-8-单条插入数据"><a href="#4-8-单条插入数据" class="headerlink" title="4.8.  单条插入数据"></a><a href="">4.8.  单条插入数据</a></h2><p>Put</p><p>包：org.apache.hadoop.hbase.client.Put</p><p>作用：插入数据</p><p>用法：</p><p>Put put = new Put(row);</p><p>p.add(family,qualifier,value);</p><p>说明：向表 tablename 添加“family,qualifier,value”指定的值。</p><p>示例代码：</p><p>HTablePool pool = new HTablePool(config,1000);</p><p>HTableInterface  table = pool.getTable(tableName);</p><p>Put put = new Put(Bytes.toBytes(rowKey));</p><p>put.add(Bytes.toBytes(family),Bytes.toBytes(qualifier),Bytes.toBytes(value));</p><p>table.put(put);</p><h2 id="4-9-批量插入"><a href="#4-9-批量插入" class="headerlink" title="4.9.  批量插入"></a><a href="">4.9.  批量插入</a></h2><p>批量插入</p><p>List<put> list = newArrayList<put>();</put></put></p><p>Put put = new Put(Bytes.toBytes(rowKey));//获取put，用于插入</p><p>put.add(Bytes.toBytes(family),Bytes.toBytes(qualifier),Bytes.toBytes(value));//封装信息</p><p>list.add(put);</p><p>table.put(list);//添加记录</p><h2 id="4-10-删除数据"><a href="#4-10-删除数据" class="headerlink" title="4.10.   删除数据"></a><a href="">4.10.   删除数据</a></h2><p>Delete</p><p>包：org.apache.hadoop.hbase.client.Delete</p><p>作用：删除给定rowkey的数据</p><p>用法：</p><p>Delete del= newDelete(Bytes.toBytes(rowKey));</p><p>table.delete(del);</p><p>代码实例</p><p>HTablePool pool = new HTablePool(config,1000);</p><p>HTableInterface  table = pool.getTable(tableName);</p><p>Delete del= newDelete(Bytes.toBytes(rowKey));</p><p>table.delete(del);</p><h2 id="4-11-单条查询"><a href="#4-11-单条查询" class="headerlink" title="4.11.   单条查询"></a><a href="">4.11.   单条查询</a></h2><p>Get</p><p>包：org.apache.hadoop.hbase.client.Get</p><p>作用：获取单个行的数据</p><p>用法：</p><p>HTable table = new HTable(config,Bytes.toBytes(tablename));</p><p>Get get = new Get(Bytes.toBytes(row));</p><p>Result result = table.get(get);</p><p>说明：获取 tablename 表中 row 行的对应数据</p><p>代码示例：</p><p>HTablePool pool = new HTablePool(config,1000);</p><p>HTableInterface  table = pool.getTable(tableName);</p><p>Get get = new Get(rowKey.getBytes());</p><p>Result row = table.get(get);</p><p>for (KeyValue kv : row.raw()) {</p><p>​         System.out.print(newString(kv.getRow()) + “ “);</p><p>​         System.out.print(newString(kv.getFamily()) + “:”);</p><p>​         System.out.print(newString(kv.getQualifier()) + “ = “);</p><p>​         System.out.print(newString(kv.getValue()));</p><p>​         System.out.print(“timestamp = “ + kv.getTimestamp() + “\n”);</p><p>}</p><h2 id="4-12-批量查询"><a href="#4-12-批量查询" class="headerlink" title="4.12.   批量查询"></a><a href="">4.12.   批量查询</a></h2><p>ResultScanner</p><p>包：org.apache.hadoop.hbase.client.ResultScanner</p><p>作用：获取值的接口</p><p>用法：</p><p>ResultScanner scanner =table.getScanner(scan);</p><p>For(Result rowResult : scanner){</p><p>​       Bytes[] str = rowResult.getValue(family,column);</p><p>}</p><p>说明：循环获取行中列值。</p><p>代码示例：</p><p>HTablePool pool = new HTablePool(config,1000);</p><p>HTableInterface table =pool.getTable(tableName);</p><p>Scan scan = new Scan();</p><p>scan.setStartRow(“a1”.getBytes());</p><p>scan.setStopRow(“a20”.getBytes());</p><p>ResultScanner scanner =table.getScanner(scan);</p><p>for (Result row : scanner) {</p><p>​         System.out.println(“\nRowkey:” + new String(row.getRow()));</p><p>​         for(KeyValue kv : row.raw()) {</p><p>​              System.out.print(new String(kv.getRow()) +” “);</p><p>​              System.out.print(newString(kv.getFamily()) + “:”);</p><p>​              System.out.print(newString(kv.getQualifier()) + “ = “);</p><p>​              System.out.print(newString(kv.getValue()));</p><p>​              System.out.print(“ timestamp = “+ kv.getTimestamp() + “\n”);</p><p>​         }</p><p>}</p><h2 id="4-13-hbase过滤器"><a href="#4-13-hbase过滤器" class="headerlink" title="4.13.   hbase过滤器"></a><a href="">4.13.   hbase</a>过滤器</h2><h3 id="4-13-1-FilterList"><a href="#4-13-1-FilterList" class="headerlink" title="4.13.1.           FilterList"></a><a href="">4.13.1.           FilterList</a></h3><p>FilterList 代表一个过滤器列表，可以添加多个过滤器进行查询，多个过滤器之间的关系有：</p><p>与关系（符合所有）：FilterList.Operator.MUST_PASS_ALL</p><p>或关系（符合任一）：FilterList.Operator.MUST_PASS_ONE</p><p>使用方法：</p><p>FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ONE);   </p><p>Scan s1 = new Scan();  </p><p> filterList.addFilter(new SingleColumnValueFilter(Bytes.toBytes(“f1”),  Bytes.toBytes(“c1”),  CompareOp.EQUAL,Bytes.toBytes(“v1”) )  );  </p><p>filterList.addFilter(new SingleColumnValueFilter(Bytes.toBytes(“f1”),  Bytes.toBytes(“c2”),  CompareOp.EQUAL,Bytes.toBytes(“v2”) )  );  </p><p> // 添加下面这一行后，则只返回指定的cell，同一行中的其他cell不返回  </p><p> s1.addColumn(Bytes.toBytes(“f1”), Bytes.toBytes(“c1”));  </p><p> s1.setFilter(filterList);  //设置filter</p><p> ResultScanner ResultScannerFilterList = table.getScanner(s1);  //返回结果列表</p><h3 id="4-13-2-过滤器的种类"><a href="#4-13-2-过滤器的种类" class="headerlink" title="4.13.2.           过滤器的种类"></a><a href="">4.13.2.           过滤器的种类</a></h3><p>过滤器的种类：</p><p>列值过滤器—SingleColumnValueFilter </p><p>​     过滤列植的相等、不等、范围等</p><p>列名前缀过滤器—ColumnPrefixFilter </p><p>​     过滤指定前缀的列名</p><p>多个列名前缀过滤器—MultipleColumnPrefixFilter</p><p>​      过滤多个指定前缀的列名</p><p>rowKey过滤器—RowFilter</p><p>​     通过正则，过滤rowKey值。</p><h3 id="4-13-3-列植过滤器—SingleColumnValueFilter"><a href="#4-13-3-列植过滤器—SingleColumnValueFilter" class="headerlink" title="4.13.3.           列植过滤器—SingleColumnValueFilter"></a><a href="">4.13.3.           列植过滤器—SingleColumnValueFilter</a></h3><p>SingleColumnValueFilter 列值判断</p><p>相等 (CompareOp.EQUAL ), </p><p>不等(CompareOp.NOT_EQUAL),</p><p>范围 (e.g., CompareOp.GREATER)…………</p><p>下面示例检查列值和字符串’values’ 相等…</p><p>SingleColumnValueFilter f = new  SingleColumnValueFilter(</p><p>​                            Bytes.toBytes(“cFamily”)                                                Bytes.toBytes(“column”),                      CompareFilter.CompareOp.EQUAL,</p><p>​       Bytes.toBytes(“values”));</p><p>s1.setFilter(f);</p><p>注意：如果过滤器过滤的列在数据表中有的行中不存在，那么这个过滤器对此行无法过滤。</p><h3 id="4-13-4-列名前缀过滤器—ColumnPrefixFilter"><a href="#4-13-4-列名前缀过滤器—ColumnPrefixFilter" class="headerlink" title="4.13.4.           列名前缀过滤器—ColumnPrefixFilter"></a><a href="">4.13.4.           列名前缀过滤器—ColumnPrefixFilter</a></h3><p>过滤器—ColumnPrefixFilter </p><p>ColumnPrefixFilter 用于指定列名前缀值相等</p><p>ColumnPrefixFilter f = newColumnPrefixFilter(Bytes.toBytes(“values”));</p><p>s1.setFilter(f);</p><h3 id="4-13-5-多个列值前缀过滤器—MultipleColumnPrefixFilter"><a href="#4-13-5-多个列值前缀过滤器—MultipleColumnPrefixFilter" class="headerlink" title="4.13.5.           多个列值前缀过滤器—MultipleColumnPrefixFilter"></a><a href="">4.13.5.           多个列值前缀过滤器—MultipleColumnPrefixFilter</a></h3><p>MultipleColumnPrefixFilter 和ColumnPrefixFilter 行为差不多，但可以指定多个前缀</p><p>byte[][] prefixes = new byte[][]{Bytes.toBytes(“value1”),Bytes.toBytes(“value2”)};</p><p>Filter f = newMultipleColumnPrefixFilter(prefixes);</p><p>s1.setFilter(f);</p><h3 id="4-13-6-rowKey过滤器—RowFilter"><a href="#4-13-6-rowKey过滤器—RowFilter" class="headerlink" title="4.13.6.           rowKey过滤器—RowFilter"></a><a href="">4.13.6.           rowKey</a>过滤器—RowFilter</h3><p>RowFilter 是rowkey过滤器</p><p>通常根据rowkey来指定范围时，使用scan扫描器的StartRow和StopRow方法比较好。</p><p>Filter f = newRowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(“^1234”));//匹配以1234开头的rowkey</p><p>s1.setFilter(f);</p>]]></content>
    
    <summary type="html">
    
      Hbase开发入门指南
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hbase" scheme="dengcongbao.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce运行原理详解</title>
    <link href="dengcongbao.github.io/2018/01/17/MapReduce%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/"/>
    <id>dengcongbao.github.io/2018/01/17/MapReduce运行原理详解/</id>
    <published>2018-01-17T07:08:45.000Z</published>
    <updated>2018-01-17T08:01:59.073Z</updated>
    
    <content type="html"><![CDATA[<p>我们通过提交jar包，进行MapReduce处理，那么整个运行过程分为五个环节：</p><p>　　1、向client端提交MapReduce job.</p><p>　　2、随后yarn的ResourceManager进行资源的分配.</p><p>　　3、由NodeManager进行加载与监控containers.</p><p>　　4、通过applicationMaster与ResourceManager进行资源的申请及状态的交互，由NodeManagers进行MapReduce运行时job的管理.</p><p>　　5、通过hdfs进行job配置文件、jar包的各节点分发。</p><p>​       <img src="/img/mapreduce104.jpg" alt="mapreduce"></p><p><strong>Job 提交过程</strong></p><p>　　job的提交通过<strong>调用submit()方法</strong>创建一个<strong>JobSubmitter</strong>实例，并<strong>调用submitJobInternal()</strong>方法。整个job的运行过程如下：</p><p>　　1、向ResourceManager申请application ID，此ID为该MapReduce的jobId。</p><p>　　2、检查output的路径是否正确，是否已经被创建。</p><p>　　3、计算input的splits。</p><p>　　4、拷贝运行job 需要的jar包、配置文件以及计算input的split 到各个节点。</p><p>　　5、在ResourceManager中调用submitAppliction()方法,执行job</p><p><strong>Job 初始化过程</strong></p><p>　　1、当resourceManager收到了submitApplication()方法的调用通知后，scheduler开始分配container,随之ResouceManager发送applicationMaster进程，告知每个nodeManager管理器。</p><p>　　2、<strong>由applicationMaster决定</strong>如何运行tasks,如果job数据量比较小，applicationMaster便选择<strong>将tasks运行在一个JVM中</strong>。那么如何判别这个job是大是小呢？当一个job的<strong>mappers数量小于10个</strong>，<strong>只有一个reducer或者读取的文件大小要小于一个HDFS block时</strong>，（可通过修改配置项mapreduce.job.ubertask.maxmaps,mapreduce.job.ubertask.maxreduces以及mapreduce.job.ubertask.maxbytes 进行调整)</p><p>　　3、在运行tasks之前，applicationMaster将会<strong>调用setupJob()方法</strong>，随之创建output的输出路径(这就能够解释，不管你的mapreduce一开始是否报错，输出路径都会创建)</p><p><strong>Task 任务分配</strong></p><p>　　1、接下来applicationMaster向ResourceManager请求containers用于执行map与reduce的tasks（step 8),这里map task的优先级要高于reduce task，当所有的map tasks结束后，随之进行sort(这里是shuffle过程后面再说）,最后进行reduce task的开始。(这里有一点，当map tasks执行了百分之5%的时候，将会请求reduce，具体下面再总结)</p><p>　　2、运行tasks的是需要消耗内存与CPU资源的，<strong>默认情况下，map和reduce的task资源分配为1024MB与一个核</strong>，（可修改运行的最小与最大参数配置,mapreduce.map.memory.mb,mapreduce.reduce.memory.mb,mapreduce.map.cpu.</p><p>vcores,mapreduce.reduce.reduce.cpu.vcores.)</p><p><strong>Task 任务执行</strong></p><p>　　1、这时一个task已经被ResourceManager分配到一个container中，由applicationMaster告知nodemanager启动container，这个task将会被一个<strong>主函数为YarnChild</strong>的java application运行，但在运行task之前，<strong>首先定位task需要的jar包、配置文件以及加载在缓存中的文件</strong>。</p><p>　　2、YarnChild运行于一个专属的JVM中，所以<strong>任何一个map或reduce任务出现问题，都不会影响整个nodemanager的crash或者hang</strong>。</p><p>　　3、每个task都可以在相同的JVM task中完成，随之将完成的处理数据写入临时文件中。</p><p>Mapreduce数据流</p><p><strong>运行进度与状态更新</strong></p><p>　　1、MapReduce是一个较长运行时间的批处理过程，可以是一小时、几小时甚至几天，那么Job的运行状态监控就非常重要。每个job以及<strong>每个task都有一个包含job（running,successfully completed,failed）的状态</strong>，以及value的计数器，状态信息及描述信息（描述信息一般都是在代码中加的打印信息），那么，这些信息是如何与客户端进行通信的呢？</p><p>　　2、当一个task开始执行，它将会保持运行记录，记录task完成的比例，对于map的任务，将会记录其运行的百分比，对于reduce来说可能复杂点，但系统依旧会估计reduce的完成比例。当一个map或reduce任务执行时，<strong>子进程会持续每三秒钟与applicationMaster进行交互</strong>。</p><p><strong>Job 完成</strong></p><p> 　　最终，applicationMaster会收到一个job完成的通知，随后改变job的状态为successful。最终，applicationMaster与task containers被清空。</p><p><strong>Shuffle与Sort</strong></p><p>　　从map到reduce的过程，被称之为shuffle过程，MapReduce使到reduce的数据一定是经过key的排序的，那么shuffle是如何运作的呢？</p><p>　　当map任务将数据output时，<strong>不仅仅是将结果输出到磁盘，它是将其写入内存缓冲区域，并进行一些预分类</strong>。</p><p><img src="/img/suffle-sort.jpg" alt="shuffle"></p><p><strong>　1、The Map Side</strong></p><p>　　首先map任务的<strong>output过程是一个环状的内存缓冲区，缓冲区的大小默认为100MB</strong>（可通过修改配置项mpareduce.task.io.sort.mb进行修改），当写入内存的大小到达一定比例<strong>，默认为80%</strong>（可通过mapreduce.map.sort.spill.percent配置项修改）,便开始写入磁盘。</p><p>　　在写入磁盘之前，线程将会指定数据写入与reduce相应的patitions中，最终传送给reduce.在每个partition中<strong>，后台线程将会在内存中进行Key的排序</strong>，（<strong>如果代码中有combiner方法，则会在output时就进行sort排序</strong>，这里，如果只有少于3个写入磁盘的文件，combiner将会在outputfile前启动，如果只有一个或两个，那么将不会调用）</p><p>　 这里<strong>将map输出的结果进行压缩会大大减少磁盘IO与网络传输的开销</strong>（配置参数mapreduce.map .output.compress 设置为true,如果使用第三方压缩jar，可通过mapreduce.map.output.compress.codec进行设置)</p><p>　  随后这些paritions输出文件将会通过HTTP发送至reducers，传送的最大启动线程通过mapreduce.shuffle.max.threads进行配置。</p><p><strong>　　2、The Reduce Side</strong></p><p>　　首先上面每个节点的map都将结果写入了本地磁盘中，现在reduce需要将map的结果通过集群拉取过来，这里要注意的是，<strong>需要等到所有map任务结束后,reduce才会对map的结果进行拷贝</strong>，由于reduce函数有少数几个复制线程，以至于它<strong>可以同时拉取多个map的输出结果。默认的为5个线程</strong>（可通过修改配置mapreduce.reduce.shuffle.parallelcopies来修改其个数）</p><p>　　这里有个问题，那么reducers怎么知道从哪些机器拉取数据呢？ </p><p>　　当所有map的任务结束后，<strong>applicationMaster通过心跳机制（heartbeat mechanism)，由它知道mapping的输出结果与机器host</strong>,所以<strong>reducer会定时的通过一个线程访问applicationmaster请求map的输出结果</strong>。</p><p>　　Map的结果将会被拷贝到reduce task的JVM的内存中（内存大小可在mapreduce.reduce.shuffle.input.buffer.percent中设置）如果不够用，则会写入磁盘。当内存缓冲区的大小到达一定比例时（可通过mapreduce.reduce.shuffle.merge.percent设置)或map的输出结果文件过多时（可通过配置mapreduce.reduce.merge.inmen.threshold)，将会除法合并(merged)随之写入磁盘。</p><p>　　这时要注意，<strong>所有的map结果这时都是被压缩过的，需要先在内存中进行解压缩，以便后续合并它们</strong>。（合并最终文件的数量可通过mapreduce.task.io.sort.factor进行配置） 最终reduce进行运算进行输出。</p><p>参考文献:《Hadoop:The Definitive Guide, 4th Edition》 </p><p>本博文摘抄地址：<a href="https://www.cnblogs.com/yangsy0915/p/5559969.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangsy0915/p/5559969.html</a></p>]]></content>
    
    <summary type="html">
    
      MapReduce合并了两种经典函数：映射（Mapping）对集合里的每个目标应用同一个操作。即，如果你想把表单里每个单元格乘以二，那么把这个函数单独地应用在每个单元格上的操作就属于mapping。化简（Reducing ）遍历集合中的元素来返回一个综合的结果。即，输出表单里一列数字的和这个任务属于reducing。
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="MapReduce" scheme="dengcongbao.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive安装之单机模式</title>
    <link href="dengcongbao.github.io/2018/01/16/Hive%E5%AE%89%E8%A3%85%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F/"/>
    <id>dengcongbao.github.io/2018/01/16/Hive安装之单机模式/</id>
    <published>2018-01-16T05:11:44.000Z</published>
    <updated>2018-01-16T08:21:53.840Z</updated>
    
    <content type="html"><![CDATA[<p>①下载Hive安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.bit.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz</span><br></pre></td></tr></table></figure><p>②安装mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>1.下载mysql的repo源</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>2.安装mysql-community-release-el7-5.noarch.rpm包</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>安装这个包后，会获得两个mysql的yum repo源：</span><br><span class="line">/etc/yum.repos.d/mysql-community.repo，/etc/yum.repos.d/mysql-community-source.repo。</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>3.安装mysql</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo yum install mysql-server -y</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>根据提示安装就可以了,不过安装完成后没有密码,需要重置密码</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>4.重置mysql密码</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> mysql -u root</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>登录时有可能报这样的错：ERROR 2002 (HY000): Can‘t connect to local MySQL server through socket #‘/var/lib/mysql/mysql.sock‘ (2)，原因是/var/lib/mysql的访问权限问题。下面的命令把/var/lib/mysql的拥有#者改为当前用户：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo chown -R root:root /var/lib/mysql</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>重启mysql服务</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> service mysqld restart</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>接下来登录重置密码：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> mysql -u root  //直接回车进入mysql控制台</span><br><span class="line">mysql &gt; use mysql;</span><br><span class="line">mysql &gt; update user set password=password('123456') where user='root';</span><br><span class="line">mysql &gt; exit;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>再次重启mysql</span><br><span class="line"><span class="meta">$</span> service mysqld restart</span><br><span class="line"><span class="meta">$</span> mysql -u root -p </span><br><span class="line">Enter password: 此处输入密码</span><br></pre></td></tr></table></figure><p>③安装Hive</p><p>(1）解压Hive压缩包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.2-bin.tar.gz -C /usr/local</span><br><span class="line"></span><br><span class="line">cd /usr/local</span><br><span class="line"></span><br><span class="line">mv apache-hive-1.2.2-bin hive-1.2.2</span><br></pre></td></tr></table></figure><p>(2）修改配置文件</p><p>配置文件重命名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive-1.2.2</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line">cp hive-default.xml.template hive-site.xml</span><br><span class="line">cp hive-log4j2.properties.template hive-log4j2.properties</span><br><span class="line">cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties</span><br></pre></td></tr></table></figure><p>配置文件修改</p><p>hive-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HEAPSIZE=1024</span><br><span class="line">HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.2</span><br><span class="line">export HIVE_CONF_DIR=/usr/local/hive-1.2.2/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/usr/local/hive-1.2.2/lib</span><br></pre></td></tr></table></figure><p>(3)创建HDFS目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hive/warehouse</span><br><span class="line">hdfs dfs -mkdir -p /user/hive/tmp</span><br><span class="line">hdfs dfs -mkdir -p /user/hive/log</span><br><span class="line">hdfs dfs -chmod g+w /user/hive/warehouse</span><br><span class="line">hdfs dfs -chmod g+w /usr/hive/tmp</span><br><span class="line">hdfs dfs -chmod g+w /usr/hive/log</span><br></pre></td></tr></table></figure><p>(4)创建数据库和hive用户</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">假定你已经安装好 MySQL。下面创建一个 hive 数据库用来存储 Hive 元数据，且数据库访问的用户名和密码都为 hive。</span><br><span class="line"></span><br><span class="line">mysql&gt; CREATE DATABASE hive; </span><br><span class="line">mysql&gt; USE hive; </span><br><span class="line">mysql&gt; CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';</span><br><span class="line">mysql&gt; GRANT ALL ON hive.* TO 'hive'@'localhost' IDENTIFIED BY 'hive'; </span><br><span class="line">mysql&gt; GRANT ALL ON hive.* TO 'hive'@'%' IDENTIFIED BY 'hive'; </span><br><span class="line">mysql&gt; FLUSH PRIVILEGES; </span><br><span class="line">mysql&gt; quit;</span><br><span class="line">修改hive-site.xml</span><br><span class="line">需要在 hive-site.xml 文件中配置 MySQL 数据库连接信息。</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">运行Hive</span><br><span class="line">在命令行运行 hive 命令时必须保证以下两点：</span><br><span class="line"></span><br><span class="line">HDFS 已经启动。可以使用 start-dfs.sh 脚本来启动 HDFS。</span><br><span class="line">MySQL Java 连接器添加到 $HIVE_HOME/lib 目录下。我安装时使用的是 mysql-connector-java-5.1.39.jar。</span><br><span class="line">从 Hive 2.1 版本开始, 我们需要先运行 schematool 命令来执行初始化操作。</span><br><span class="line"></span><br><span class="line">schematool -dbType mysql -initSchema</span><br><span class="line"></span><br><span class="line">报错：缺少mysql jar包</span><br><span class="line">解决：将其（如mysql-connector-Java-5.1.15-bin.jar）拷贝到$HIVE_HOME/lib下即可。</span><br></pre></td></tr></table></figure><p>启动Hive时报以下错误:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span>java.lang.RuntimeException: java.lang.IllegalArgumentException:java.net.URISyntaxException: Relative path in absolute URI:$&#123;system:java.io.tmpdir%<span class="number">7</span>D/$%<span class="number">7</span>Bsystem:user.name%<span class="number">7</span>D</span><br><span class="line">        atorg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">444</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:<span class="number">672</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:<span class="number">616</span>)</span><br><span class="line">        atsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        atsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">57</span>)</span><br><span class="line">        atsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">        atjava.lang.reflect.Method.invoke(Method.java:<span class="number">606</span>)</span><br><span class="line">        atorg.apache.hadoop.util.RunJar.main(RunJar.java:<span class="number">160</span>)</span><br><span class="line">Caused by: java.lang.IllegalArgumentException:java.net.URISyntaxException: Relative path in absolute URI:$&#123;system:java.io.tmpdir%<span class="number">7</span>D/$%<span class="number">7</span>Bsystem:user.name%<span class="number">7</span>D</span><br><span class="line">        atorg.apache.hadoop.fs.Path.initialize(Path.java:<span class="number">148</span>)</span><br><span class="line">        atorg.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:<span class="number">126</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:<span class="number">487</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">430</span>)</span><br><span class="line">        ... <span class="number">7</span>more</span><br></pre></td></tr></table></figure><p>解决方案：将 hive-site.xml 中的 ${system:java.io.tmpdir}  和  ${system:user.name} 分别替换成 /tmp 和 ${user.name}</p>]]></content>
    
    <summary type="html">
    
      Hive 是建立在 Hadoop  上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL ），这是一种可以存储、查询和分析存储在 Hadoop  中的大规模数据的机制。Hive 定义了简单的类 SQL  查询语言，称为HQL ，它允许熟悉 SQL  的用户查询数据。同时，这个语言也允许熟悉 MapReduce  开发者的开发自定义的 mapper  和 reducer  来处理内建的 mapper 和 reducer  无法完成的复杂的分析工作。Hive是SQL解析引擎，它将SQL语句转译成M/R Job然后在Hadoop执行。Hive的表其实就是HDFS的目录/文件，按表名把文件夹分开。如果是分区表，则分区值是子文件夹，可以直接在M/R Job里使用这些数据。
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hive" scheme="dengcongbao.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Mahout安装之单机模式</title>
    <link href="dengcongbao.github.io/2018/01/16/Mahout%E5%AE%89%E8%A3%85%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F/"/>
    <id>dengcongbao.github.io/2018/01/16/Mahout安装之单机模式/</id>
    <published>2018-01-16T02:36:36.000Z</published>
    <updated>2018-01-16T02:37:35.923Z</updated>
    
    <summary type="html">
    
      Mahout是一个算法库,集成了很多算法。Apache Mahout 是 Apache Software Foundation（ASF）旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout项目已经发展到了它的第4个年头，目前已经有了多个公共发行版本。Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。 Mahout 的创始人 Grant Ingersoll 介绍了机器学习的基本概念，并演示了如何使用 Mahout 来实现文档集群、提出建议和组织内容。
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hbase" scheme="dengcongbao.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase之单机模式安装</title>
    <link href="dengcongbao.github.io/2018/01/16/Hbase%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85/"/>
    <id>dengcongbao.github.io/2018/01/16/Hbase之单机模式安装/</id>
    <published>2018-01-16T02:01:17.000Z</published>
    <updated>2018-01-16T05:33:54.704Z</updated>
    
    <content type="html"><![CDATA[<p>①下载hbase安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.bit.edu.cn/apache/hbase/1.2.6/hbase-1.2.6-bin.tar.gz</span><br></pre></td></tr></table></figure><p>②解压安装包到指定目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/local/hbase</span><br><span class="line">tar -zxvf hbase-1.2.6-bin.tar.gz -C /usr/local/hbase</span><br></pre></td></tr></table></figure><p>③修改配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hbase/hbase-1.2.6/conf</span><br><span class="line"></span><br><span class="line">vi hbase-site.xml</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.19.128:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">vi hbase-env.sh</span><br><span class="line"># hbase-env.sh添加JAVA_HOME配置</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_131/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line">HBASE_HOME=/usr/local/hbase/hbase-1.2.6</span><br><span class="line">PATH=$HBASE_HOME/bin:$PATH</span><br><span class="line">export HBASE_HOME PATH</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>④启动Hbase</p><p>启动hadoop :       start-all.sh</p><p>启动zookeeper： zkServer.sh start</p><p>启动Hbase:          start-hbase.sh</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;①下载hbase安装包&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;c
      
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hbase" scheme="dengcongbao.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Java的native方法</title>
    <link href="dengcongbao.github.io/2018/01/13/Java%E7%9A%84native%E6%96%B9%E6%B3%95/"/>
    <id>dengcongbao.github.io/2018/01/13/Java的native方法/</id>
    <published>2018-01-12T16:23:43.000Z</published>
    <updated>2018-01-12T17:42:33.967Z</updated>
    
    <summary type="html">
    
      native方法称为本地方法。在java源程序中以关键字“native”声明，不提供函数体。
    
    </summary>
    
      <category term="Java" scheme="dengcongbao.github.io/categories/Java/"/>
    
    
      <category term="Java虚拟机" scheme="dengcongbao.github.io/tags/Java%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.0.0单机模式安装</title>
    <link href="dengcongbao.github.io/2018/01/12/Spark2-0-0%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85/"/>
    <id>dengcongbao.github.io/2018/01/12/Spark2-0-0单机模式安装/</id>
    <published>2018-01-12T12:46:53.000Z</published>
    <updated>2018-01-18T02:21:43.847Z</updated>
    
    <content type="html"><![CDATA[<p>①解压已下载的spark压缩包</p><p>tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz -C /usr/local/spark/spark-2.0.0-bin-hadoop2.7</p><p>②配置文件修改</p><p>/conf spark-env.sh末尾加上 export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/hadoop-2.7.2/bin/hadoop classpath)</p><p>②配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME=/usr/local/spark/spark-2.0.0-bin-hadoop2.7</span><br><span class="line">PATH=$SPARK_HOME/bin:$PATH </span><br><span class="line">export SPARK_HOME PATH</span><br></pre></td></tr></table></figure><p>③启动spark   ./sbin/start-master.sh</p><p>命令：关闭spark : ./sbin/stop-master.sh</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;①解压已下载的spark压缩包&lt;/p&gt;
&lt;p&gt;tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz -C /usr/local/spark/spark-2.0.0-bin-hadoop2.7&lt;/p&gt;
&lt;p&gt;②配置文件修改&lt;/p&gt;
&lt;p&gt;/conf 
      
    
    </summary>
    
      <category term="Spark系列" scheme="dengcongbao.github.io/categories/Spark%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="Spark" scheme="dengcongbao.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper安装之单机模式</title>
    <link href="dengcongbao.github.io/2018/01/12/Zookeeper%E5%AE%89%E8%A3%85%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F/"/>
    <id>dengcongbao.github.io/2018/01/12/Zookeeper安装之单机模式/</id>
    <published>2018-01-12T12:21:12.000Z</published>
    <updated>2019-01-16T14:39:15.616Z</updated>
    
    <content type="html"><![CDATA[<p>①下载zookeeper</p><p>wget <a href="http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz</a></p><p>②解压 zookeeper到指定目录</p><p>mkdir /usr/local/zookeeper</p><p>tar -zxvf zookeeper-3.3.6.tar.gz -C /usr/local/zookeeper/</p><p>③复制 zoo_sample.cfg 为zoo.cfg                   </p><p>cd /usr/local/zookeeper/zookeeper-3.3.6/conf/</p><p>cp zoo_sample.cfg zoo.cfg</p><p>④设置zookeeper环境变量</p><p>vi /etc/profile</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZOOKEEPER_HOME=/usr/<span class="built_in">local</span>/zookeeper/zookeeper-3.3.6</span><br><span class="line">PATH=<span class="variable">$ZOOKEEPER_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME PATH</span><br></pre></td></tr></table></figure><p>source /etc/profile</p><p>④启动zookeeper</p><p>zkServer.sh start  启动命令  |    zkServer.sh status 查看状态</p><p>⑤客户端连接</p><p>zkCli.sh -server 192.168.19.128:2181 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;①下载zookeeper&lt;/p&gt;
&lt;p&gt;wget &lt;a href=&quot;http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="zookeeper" scheme="dengcongbao.github.io/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Spark之WorkCount</title>
    <link href="dengcongbao.github.io/2018/01/11/Spark%E4%B9%8BWorkCount/"/>
    <id>dengcongbao.github.io/2018/01/11/Spark之WorkCount/</id>
    <published>2018-01-11T13:44:27.000Z</published>
    <updated>2018-01-18T02:22:11.629Z</updated>
    
    <content type="html"><![CDATA[<p>①使用IntelliJ IDEA新建Scala-maven项目</p><p>②Spark workcount程序编写</p><p> 第一步配置pom.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.spark.test<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inceptionYear</span>&gt;</span>2008<span class="tag">&lt;/<span class="name">inceptionYear</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-make:transitive<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-dependencyfile<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;project.build.directory&#125;/.scala_dependencies<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span>                   <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>guiguzi.spark.test.Test<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p>第二步：编写workcount程序</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> guiguzi.spark.test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      conf.setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      <span class="keyword">val</span> file = <span class="string">"hdfs://192.168.19.128:9000/user/hadoop/1.txt"</span></span><br><span class="line">      <span class="keyword">val</span> lines = sc.textFile(file)</span><br><span class="line">      <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      <span class="keyword">val</span> wordCount = words.countByValue()</span><br><span class="line">      println(wordCount)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>③把工程打成jar包 上传到服务器</p><p>双击下图的package，项目会生成相应的jar,将jar上传到服务器即可。</p><p><img src="/img/package01.png" alt="截图"></p><p>④启动spark 启动程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">启动程序命令：spark-submit --class guiguzi.spark.test.Test spark-test-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>有类似输出表明程序执行成功：Map(19 -&gt; 1, 192 -&gt; 3, 128 -&gt; 1, 12 -&gt; 1)</p>]]></content>
    
    <summary type="html">
    
      Spark之WorkCount
    
    </summary>
    
      <category term="Spark系列" scheme="dengcongbao.github.io/categories/Spark%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="WorkCount" scheme="dengcongbao.github.io/tags/WorkCount/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop2.7.1单机环境安装</title>
    <link href="dengcongbao.github.io/2018/01/11/Hadoop2-7%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"/>
    <id>dengcongbao.github.io/2018/01/11/Hadoop2-7单机环境安装/</id>
    <published>2018-01-11T07:08:31.000Z</published>
    <updated>2018-01-12T12:20:28.030Z</updated>
    
    <content type="html"><![CDATA[<p>下载一份Hadoop.tar.gz<br>解压到你想要的目录， 我这里放到/usr/local/hadoop下。<br>修改如下配置文件：</p><p>cd  /usr/local/hadoop/hadoop-2.7.2/etc/hadoop/</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">core-site.xml:</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.19.128:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">hdfs-site.xml:</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.support.append<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>ssh密钥：<br>$ ssh-keygen -t dsa -P ‘’ -f ~/.ssh/id_dsa<br>$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>配置conf下hadoop-env.sh:<br>export JAVA_HOME=/usr/java/jdk1.8.0_131 #配置java_home<br>格式化namenode：<br>bin/hadoop namenode -format</p><p>增加环境变量：vi /etc/profile</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib.tools.jar</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export JAVA_HOME CLASSPATH PATH</span><br><span class="line">HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.2</span><br><span class="line">PATH=$HADOOP_HOME/bin:$PATH</span><br><span class="line">export HADOOP_HOME PATH</span><br></pre></td></tr></table></figure><p>生效环境变量：source /etc/profile</p><p>开启服务：<br>bin/start-all.sh </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;下载一份Hadoop.tar.gz&lt;br&gt;解压到你想要的目录， 我这里放到/usr/local/hadoop下。&lt;br&gt;修改如下配置文件：&lt;/p&gt;
&lt;p&gt;cd  /usr/local/hadoop/hadoop-2.7.2/etc/hadoop/&lt;/p&gt;
&lt;figure c
      
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="dengcongbao.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark从外部读取数据之textFile</title>
    <link href="dengcongbao.github.io/2018/01/11/Spark%E4%BB%8E%E5%A4%96%E9%83%A8%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8BtextFile/"/>
    <id>dengcongbao.github.io/2018/01/11/Spark从外部读取数据之textFile/</id>
    <published>2018-01-11T06:52:18.000Z</published>
    <updated>2018-01-18T02:21:31.197Z</updated>
    
    <content type="html"><![CDATA[<p>textFile函数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * Read a text file from HDFS, a local file system (available on all nodes), or any </span></span><br><span class="line"><span class="comment"> * Hadoop-supported file system URI, and return it as an RDD of Strings. </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(  </span><br><span class="line">    path: <span class="type">String</span>,  </span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;  </span><br><span class="line">    assertNotStopped()  </span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],  </span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分析参数：</p><p>path: String 是一个URI，這个URI可以是HDFS、本地文件（全部的节点都可以），或者其他Hadoop支持的文件系统URI返回的是一个字符串类型的RDD，也就是是RDD的内部形式是Iterator[(String)]</p><p>minPartitions=  math.min(defaultParallelism, 2) 是指定数据的分区，如果不指定分区，当你的核数大于2的时候，不指定分区数那么就是 2</p><p>当你的数据大于128M时候，Spark是为每一个快（block）创建一个分片（Hadoop-2.X之后为128m一个block）</p><p>1、从当前目录读取一个文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"Current.txt"</span>  <span class="comment">//Current fold file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>从当前目录读取一个Current.txt的文件</p><p>2、从当前目录读取多个文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"Current1.txt，Current2.txt，"</span>  <span class="comment">//Current fold file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>从当前读取两个文件，分别是Cuttent1.txt和Current2.txt</p><p>3、从本地系统读取一个文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/README.md"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>4、从本地系统读取整个文件夹</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>从本地系统中读取licenses这个文件夹下的所有文件</p><p>這里特别注意的是，比如這个文件夹下有35个文件，上面分区数设置是2，那么整个RDD的分区数是35*2？</p><p>這是错误的，這个RDD的分区数不管你的partition数设置为多少时，只要license這个文件夹下的這个文件a.txt</p><p>(比如有a.txt)没有超过128m，那么a.txt就只有一个partition。那么就是说只要这35个文件其中没有一个超过</p><p>128m，那么分区数就是 35个</p><p>5、从本地系统读取多个文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/LICENSE-scala.txt,file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/LICENSE-spire.txt"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>从本地系统中读取file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/下的LICENSE-spire.txt和</p><p>LICENSE-scala.txt两个文件。上面分区设置是2，那个RDD的整个分区数是2*2</p><p>6、从本地系统读取多个文件夹下的文件（把如下文件全部读取进来）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"/usr/local/spark/spark-1.6.0-bin-hadoop2.6/data/*/*"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>采用通配符的形式来代替文件，来对数据文件夹进行整体读取。但是后面设置的分区数2也是可以去除的。因为一个文件没有达到128m，所以上面的一个文件一个partition，一共是20个。</p><p>7采用通配符，来读取多个文件名类似的文件</p><p>比如读取如下文件的people1.txt和people2.txt,但google.txt不读取</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">2</span>)&#123;  </span><br><span class="line">      <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">s"/root/application/temp/people<span class="subst">$i</span>*"</span>,<span class="number">2</span>)  </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>8、采用通配符读取相同后缀的文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"/usr/local/spark/spark-1.6.0-bin-hadoop2.6/data/*/*.txt"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>9、从HDFS读取一个文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"hdfs://master:9000/examples/examples/src/main/resources/people.txt"</span>  </span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>从HDFS中读取文件的形式和本地上一样，只是前面的路径要表明是HDFS中的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;textFile函数&lt;/p&gt;
&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/sp
      
    
    </summary>
    
      <category term="Spark系列" scheme="dengcongbao.github.io/categories/Spark%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="Spark" scheme="dengcongbao.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Centos安装Hadoop3.0</title>
    <link href="dengcongbao.github.io/2018/01/06/centos-hadoop/centos-hadoop/"/>
    <id>dengcongbao.github.io/2018/01/06/centos-hadoop/centos-hadoop/</id>
    <published>2018-01-06T11:29:16.000Z</published>
    <updated>2018-01-12T02:02:43.519Z</updated>
    
    <content type="html"><![CDATA[<p>1、创建目录 解压文件到指定目录</p><p>mkdir /usr/hadoop</p><p>tar -zxvf hadoop.tar.gz -C /usr/hadoop</p><p>2、设置hadoop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">一共需要配置主要的6个文件</span><br><span class="line">    hadoop-3.0.0/etc/hadoop/hadoop-env.sh </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/yarn-env.sh </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/core-site.xml </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/hdfs-site.xml </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/mapred-site.xml </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><p>1）配置hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=$&#123;JAVA_HOME&#125;</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line"></span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_SECURE_USER=root</span><br><span class="line">export HDFS_DATANODE_SECURE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure><p>​     2）配置yarn-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p>3）配置core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.19.128:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>HDFS的URI，文件系统://namenode标识:端口号<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode上本地的hadoop临时文件夹<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>4），配置hdfs-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">!—hdfs-site.xml--</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode上存储hdfs名字空间元数据 <span class="tag">&lt;/<span class="name">description</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode上数据块的物理存储位置<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>副本个数，配置默认是3,应小于datanode机器数量<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>5）配置mapred-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>6）配置yarn-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs:192.168.19.128:8099<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>这个地址是mr管理界面的<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>3、启动及测试<br>①格式化namenode,</p><p>cd /usr/hadoop/hadoop-3.0.0    ./bin/hdfs namenode -format</p><p>②启动</p><p>cd /usr/hadoop/hadoop-3.0.0 ./sbin/start-all.sh</p><p>③访问 <a href="http://192.168.19.128:9870" target="_blank" rel="noopener">http://192.168.19.128:9870</a></p><p>4、设置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">HADOOP_HOME=/usr/hadoop/hadoop-3.0.0</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$PATH</span><br><span class="line">export HADOOP_HOME PATH</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1、创建目录 解压文件到指定目录&lt;/p&gt;
&lt;p&gt;mkdir /usr/hadoop&lt;/p&gt;
&lt;p&gt;tar -zxvf hadoop.tar.gz -C /usr/hadoop&lt;/p&gt;
&lt;p&gt;2、设置hadoop&lt;/p&gt;
&lt;figure class=&quot;highlight sh
      
    
    </summary>
    
      <category term="Hadoop" scheme="dengcongbao.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="dengcongbao.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Centos7安装java</title>
    <link href="dengcongbao.github.io/2018/01/06/centos-java/index/"/>
    <id>dengcongbao.github.io/2018/01/06/centos-java/index/</id>
    <published>2018-01-06T09:41:27.000Z</published>
    <updated>2018-01-12T02:02:10.271Z</updated>
    
    <content type="html"><![CDATA[<p>1、安装yum : yum -y update</p><p>2、安装上传插件 ： yum install lrzsz</p><p>3、上传已下载的 jdk tar包 jdk-8u131-linux-x64.tar.gz</p><p>​       rz -y 选择tar包</p><p>4、解压 ：tar -zxvf  jdk-8u131-linux-x64.tar.gz -C /usr/java</p><p>5、设置环境变量：</p><p>​     vi /etc/profile</p><p>​      ①在末尾添加在末尾行添加</p><p>​      ②在末尾行添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">set</span> java environment</span></span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib.tools.jar</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure><p>​      ③source /etc/profile</p><p>​      ④java -version 查看JDK版本信息，如果显示以下信息证明安装成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version "1.8.0_131"</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_131-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1、安装yum : yum -y update&lt;/p&gt;
&lt;p&gt;2、安装上传插件 ： yum install lrzsz&lt;/p&gt;
&lt;p&gt;3、上传已下载的 jdk tar包 jdk-8u131-linux-x64.tar.gz&lt;/p&gt;
&lt;p&gt;​       rz -y 选择ta
      
    
    </summary>
    
      <category term="Linux" scheme="dengcongbao.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="dengcongbao.github.io/tags/Linux/"/>
    
  </entry>
  
</feed>
