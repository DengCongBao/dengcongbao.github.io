<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Hbase开发入门指南]]></title>
      <url>/2018/01/18/Hbase%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/</url>
      <content type="html"><![CDATA[<h1 id="1-hbase数据模型"><a href="#1-hbase数据模型" class="headerlink" title="1.  hbase数据模型"></a><a href="">1.  hbase</a>数据模型</h1><h2 id="1-1-hbase数据模型"><a href="#1-1-hbase数据模型" class="headerlink" title="1.1.  hbase数据模型"></a><a href="">1.1.  hbase</a>数据模型</h2><p><img src="/img/datatype.png" alt="img"></p>
<h3 id="1-1-1-Row-Key"><a href="#1-1-1-Row-Key" class="headerlink" title="1.1.1.  Row Key"></a><a href="">1.1.1.  Row Key</a></h3><p>与nosql数据库们一样,row key是用来检索记录的主键。访问HBASE table中的行，只有三种方式：</p>
<p>1.通过单个row key访问</p>
<p>2.通过row key的range（正则）</p>
<p>3.全表扫描</p>
<p>Row key行键 (Row key)可以是任意字符串(最大长度 是 64KB，实际应用中长度一般为10-100bytes)，在HBASE内部，row key保存为字节数组。存储时，数据按照Rowkey的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</p>
<h3 id="1-1-2-Columns-Family"><a href="#1-1-2-Columns-Family" class="headerlink" title="1.1.2.  Columns Family"></a><a href="">1.1.2.  Columns Family</a></h3><p>列簇：HBASE表中的每个列，都归属于某个列族。列族是表的schema的一部分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如courses:history，courses:math都属于courses 这个列族。</p>
<h3 id="1-1-3-Cell"><a href="#1-1-3-Cell" class="headerlink" title="1.1.3.  Cell"></a><a href="">1.1.3.  Cell</a></h3><p>由{row key, columnFamily, version} 唯一确定的单元。cell中 的数据是没有类型的，全部是字节码形式存贮。</p>
<p>关键字：无类型、字节码</p>
<h3 id="1-1-4-Time-Stamp"><a href="#1-1-4-Time-Stamp" class="headerlink" title="1.1.4.  Time Stamp"></a><a href="">1.1.4.  Time Stamp</a></h3><p>HBASE 中通过rowkey和columns确定的为一个存贮单元称为cell。每个 cell都保存 着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由HBASE(在数据写入时自动 )赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</p>
<p>为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供 了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段 时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。</p>
<h1 id="2-hbase命令"><a href="#2-hbase命令" class="headerlink" title="2.  hbase命令"></a><a href="">2.  hbase</a>命令</h1><h2 id="2-1-命令的进退"><a href="#2-1-命令的进退" class="headerlink" title="2.1.  命令的进退"></a><a href="">2.1.  命令的进退</a></h2><p>1、hbase提供了一个shell的终端给用户交互</p>
<p>#$HBASE_HOME/bin/hbase shell </p>
<p>2、如果退出执行quit命令</p>
<p>#$HBASE_HOME/bin/hbase shell</p>
<p>……</p>
<p>>quit</p>
<h2 id="2-2-命令"><a href="#2-2-命令" class="headerlink" title="2.2.  命令"></a><a href="">2.2.  命令</a></h2><table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>命令表达式</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>创建表</strong></td>
<td><strong>create ‘**</strong>表名’, ‘<strong><strong>列族名1’,’</strong></strong>列族名2’,’<strong>**列族名N’</strong></td>
</tr>
<tr>
<td><strong>查看所有表</strong></td>
<td><strong>list</strong></td>
</tr>
<tr>
<td><strong>描述表</strong></td>
<td><strong>describe  **</strong>‘表名’**</td>
</tr>
<tr>
<td>判断表存在</td>
<td>exists  ‘表名’<em>**</em></td>
</tr>
<tr>
<td>判断是否禁用启用表<em>**</em></td>
<td>is_enabled ‘表名’  is_disabled ‘表名’</td>
</tr>
<tr>
<td><strong>添加记录      </strong></td>
<td><strong>put   **</strong>‘表名’, <strong><strong>‘rowKey</strong></strong>’, <strong><strong>‘列族 : </strong></strong>列‘  ,  ‘<strong>**值’</strong></td>
</tr>
<tr>
<td><strong>查看记录rowkey**</strong>下的所有数据**</td>
<td><strong>get   ‘**</strong>表名’ , ‘rowKey’**</td>
</tr>
<tr>
<td><strong>查看表中的记录总数</strong></td>
<td><strong>count   ‘**</strong>表名’**</td>
</tr>
<tr>
<td><strong>获取某个列族</strong></td>
<td>get ‘表名’,’rowkey’,’列族’<em>**</em></td>
</tr>
<tr>
<td><strong>获取某个列族的某个列</strong></td>
<td>get ‘表名’,’rowkey’,’列族：列’</td>
</tr>
<tr>
<td><strong>删除记录</strong></td>
<td><strong>delete   **</strong>‘表名’ ,<strong><strong>‘行名’ , </strong></strong>‘列族：列’**</td>
</tr>
<tr>
<td><strong>删除整行</strong></td>
<td><strong>deleteall ‘**</strong>表名’,’rowkey’**</td>
</tr>
<tr>
<td><strong>删除一张表</strong></td>
<td><strong>先要屏蔽该表，才能对该表进行删除</strong>  <strong>第一步 disable **</strong>‘表名’ <strong><strong>，第二步  drop ‘</strong></strong>表名’**</td>
</tr>
<tr>
<td><strong>清空表</strong></td>
<td><strong>truncate ‘**</strong>表名’**</td>
</tr>
<tr>
<td><strong>查看所有记录</strong></td>
<td><strong>scan “**</strong>表名”  **</td>
</tr>
<tr>
<td><strong>查看某个表某个列中所有数据</strong></td>
<td><strong>scan “**</strong>表名” ,  {COLUMNS=&gt;’<strong><strong>列族名:</strong></strong>列名’}**</td>
</tr>
<tr>
<td><strong>更新记录  </strong></td>
<td><strong>就是重写一遍，进行覆盖，hbase**</strong>没有修改，都是追加**</td>
</tr>
</tbody>
</table>
<h1 id="3-hbase依赖zookeeper"><a href="#3-hbase依赖zookeeper" class="headerlink" title="3.  hbase依赖zookeeper"></a><a href="">3.  hbase</a>依赖zookeeper</h1><p>zookeeper的作用：</p>
<p>1、  保存Hmaster的地址和backup-master地址</p>
<p>hmaster的作用：</p>
<p>a)      管理HregionServer</p>
<p>b)     做增删改查表的节点</p>
<p>c)      管理HregionServer中的表分配</p>
<p>2、  保存表-ROOT-的地址</p>
<p>hbase默认的根表，检索表。</p>
<p>3、  HRegionServer列表</p>
<p>表的增删改查数据。</p>
<p>和hdfs交互，存取数据。</p>
<h1 id="4-hbase开发"><a href="#4-hbase开发" class="headerlink" title="4.  hbase开发"></a><a href="">4.  hbase</a>开发</h1><h2 id="4-1-配置"><a href="#4-1-配置" class="headerlink" title="4.1.  配置"></a><a href="">4.1.  配置</a></h2><p>HBaseConfiguration</p>
<p>包：org.apache.hadoop.hbase.HBaseConfiguration</p>
<p>作用：通过此类可以对HBase进行配置</p>
<p>用法实例：</p>
<p>Configuration config =HBaseConfiguration.create();</p>
<p>说明：HBaseConfiguration.create() 默认会从classpath 中查找hbase-site.xml 中的配置信息，初始化 Configuration。</p>
<p>使用方法:</p>
<p>static Configuration config = null;</p>
<p>static {</p>
<p>​    config = HBaseConfiguration.create();</p>
<p>​    config.set(“hbase.zookeeper.quorum”,”slave1,slave2,slave3”);</p>
<p>​    config.set(“hbase.zookeeper.property.clientPort”,”2181”);</p>
<p>}</p>
<h2 id="4-2-表管理类"><a href="#4-2-表管理类" class="headerlink" title="4.2.  表管理类"></a><a href="">4.2.  表管理类</a></h2><p>HBaseAdmin</p>
<p>包：org.apache.hadoop.hbase.client.HBaseAdmin</p>
<p>作用：提供接口关系HBase 数据库中的表信息</p>
<p>用法：</p>
<p>HBaseAdmin admin = new HBaseAdmin(config);</p>
<h2 id="4-3-表描述类"><a href="#4-3-表描述类" class="headerlink" title="4.3.  表描述类"></a><a href="">4.3.  表描述类</a></h2><p>HTableDescriptor</p>
<p>包：org.apache.hadoop.hbase.HTableDescriptor</p>
<p>作用：HTableDescriptor 类包含了表的名字以及表的列族信息</p>
<p>​         表的schema（设计）</p>
<p>用法：</p>
<p>HTableDescriptor htd =newHTableDescriptor(tablename);</p>
<p>htd.addFamily(newHColumnDescriptor(“myFamily”));</p>
<h2 id="4-4-列族的描述类"><a href="#4-4-列族的描述类" class="headerlink" title="4.4.  列族的描述类"></a><a href="">4.4.  列族的描述类</a></h2><p>HColumnDescriptor</p>
<p>包：org.apache.hadoop.hbase.HColumnDescriptor</p>
<p>作用：HColumnDescriptor 维护列族的信息</p>
<p>用法：</p>
<p>htd.addFamily(newHColumnDescriptor(“myFamily”));</p>
<h2 id="4-5-创建表的操作"><a href="#4-5-创建表的操作" class="headerlink" title="4.5.  创建表的操作"></a><a href="">4.5.  创建表的操作</a></h2><p>CreateTable（一般我们用shell创建表）</p>
<p>static Configuration config = null;</p>
<p>static {</p>
<p>​    config = HBaseConfiguration.create();</p>
<p>​    config.set(“hbase.zookeeper.quorum”,”slave1,slave2,slave3”);</p>
<p>​    config.set(“hbase.zookeeper.property.clientPort”,”2181”);</p>
<p>}</p>
<p>HBaseAdmin admin = new HBaseAdmin(config);</p>
<p>HTableDescriptor desc = newHTableDescriptor(tableName);</p>
<p>HColumnDescriptor family1 = newHColumnDescriptor(“f1”);</p>
<p>HColumnDescriptor family2 = new HColumnDescriptor(“f2”);</p>
<p>desc.addFamily(family1);</p>
<p>desc.addFamily(family2);</p>
<p>admin.createTable(desc);</p>
<h2 id="4-6-删除表"><a href="#4-6-删除表" class="headerlink" title="4.6.  删除表"></a><a href="">4.6.  删除表</a></h2><p>HBaseAdmin admin = new HBaseAdmin(config);</p>
<p>admin.disableTable(tableName);</p>
<p>admin.deleteTable(tableName);</p>
<h2 id="4-7-创建一个表的类"><a href="#4-7-创建一个表的类" class="headerlink" title="4.7.  创建一个表的类"></a><a href="">4.7.  创建一个表的类</a></h2><p>HTable</p>
<p>包：org.apache.hadoop.hbase.client.HTable</p>
<p>作用：HTable 和 HBase 的表通信</p>
<p>用法：</p>
<p>// 普通获取表</p>
<p> HTable table = newHTable(config,Bytes.toBytes(tablename);</p>
<p>// 通过连接池获取表</p>
<p>HTablePool pool = new HTablePool(config,1000);</p>
<p>HTableInterface table =pool.getTable(tableName);</p>
<h2 id="4-8-单条插入数据"><a href="#4-8-单条插入数据" class="headerlink" title="4.8.  单条插入数据"></a><a href="">4.8.  单条插入数据</a></h2><p>Put</p>
<p>包：org.apache.hadoop.hbase.client.Put</p>
<p>作用：插入数据</p>
<p>用法：</p>
<p>Put put = new Put(row);</p>
<p>p.add(family,qualifier,value);</p>
<p>说明：向表 tablename 添加“family,qualifier,value”指定的值。</p>
<p>示例代码：</p>
<p>HTablePool pool = new HTablePool(config,1000);</p>
<p>HTableInterface  table = pool.getTable(tableName);</p>
<p>Put put = new Put(Bytes.toBytes(rowKey));</p>
<p>put.add(Bytes.toBytes(family),Bytes.toBytes(qualifier),Bytes.toBytes(value));</p>
<p>table.put(put);</p>
<h2 id="4-9-批量插入"><a href="#4-9-批量插入" class="headerlink" title="4.9.  批量插入"></a><a href="">4.9.  批量插入</a></h2><p>批量插入</p>
<p>List<put> list = newArrayList<put>();</put></put></p>
<p>Put put = new Put(Bytes.toBytes(rowKey));//获取put，用于插入</p>
<p>put.add(Bytes.toBytes(family),Bytes.toBytes(qualifier),Bytes.toBytes(value));//封装信息</p>
<p>list.add(put);</p>
<p>table.put(list);//添加记录</p>
<h2 id="4-10-删除数据"><a href="#4-10-删除数据" class="headerlink" title="4.10.   删除数据"></a><a href="">4.10.   删除数据</a></h2><p>Delete</p>
<p>包：org.apache.hadoop.hbase.client.Delete</p>
<p>作用：删除给定rowkey的数据</p>
<p>用法：</p>
<p>Delete del= newDelete(Bytes.toBytes(rowKey));</p>
<p>table.delete(del);</p>
<p>代码实例</p>
<p>HTablePool pool = new HTablePool(config,1000);</p>
<p>HTableInterface  table = pool.getTable(tableName);</p>
<p>Delete del= newDelete(Bytes.toBytes(rowKey));</p>
<p>table.delete(del);</p>
<h2 id="4-11-单条查询"><a href="#4-11-单条查询" class="headerlink" title="4.11.   单条查询"></a><a href="">4.11.   单条查询</a></h2><p>Get</p>
<p>包：org.apache.hadoop.hbase.client.Get</p>
<p>作用：获取单个行的数据</p>
<p>用法：</p>
<p>HTable table = new HTable(config,Bytes.toBytes(tablename));</p>
<p>Get get = new Get(Bytes.toBytes(row));</p>
<p>Result result = table.get(get);</p>
<p>说明：获取 tablename 表中 row 行的对应数据</p>
<p>代码示例：</p>
<p>HTablePool pool = new HTablePool(config,1000);</p>
<p>HTableInterface  table = pool.getTable(tableName);</p>
<p>Get get = new Get(rowKey.getBytes());</p>
<p>Result row = table.get(get);</p>
<p>for (KeyValue kv : row.raw()) {</p>
<p>​         System.out.print(newString(kv.getRow()) + “ “);</p>
<p>​         System.out.print(newString(kv.getFamily()) + “:”);</p>
<p>​         System.out.print(newString(kv.getQualifier()) + “ = “);</p>
<p>​         System.out.print(newString(kv.getValue()));</p>
<p>​         System.out.print(“timestamp = “ + kv.getTimestamp() + “\n”);</p>
<p>}</p>
<h2 id="4-12-批量查询"><a href="#4-12-批量查询" class="headerlink" title="4.12.   批量查询"></a><a href="">4.12.   批量查询</a></h2><p>ResultScanner</p>
<p>包：org.apache.hadoop.hbase.client.ResultScanner</p>
<p>作用：获取值的接口</p>
<p>用法：</p>
<p>ResultScanner scanner =table.getScanner(scan);</p>
<p>For(Result rowResult : scanner){</p>
<p>​       Bytes[] str = rowResult.getValue(family,column);</p>
<p>}</p>
<p>说明：循环获取行中列值。</p>
<p>代码示例：</p>
<p>HTablePool pool = new HTablePool(config,1000);</p>
<p>HTableInterface table =pool.getTable(tableName);</p>
<p>Scan scan = new Scan();</p>
<p>scan.setStartRow(“a1”.getBytes());</p>
<p>scan.setStopRow(“a20”.getBytes());</p>
<p>ResultScanner scanner =table.getScanner(scan);</p>
<p>for (Result row : scanner) {</p>
<p>​         System.out.println(“\nRowkey:” + new String(row.getRow()));</p>
<p>​         for(KeyValue kv : row.raw()) {</p>
<p>​              System.out.print(new String(kv.getRow()) +” “);</p>
<p>​              System.out.print(newString(kv.getFamily()) + “:”);</p>
<p>​              System.out.print(newString(kv.getQualifier()) + “ = “);</p>
<p>​              System.out.print(newString(kv.getValue()));</p>
<p>​              System.out.print(“ timestamp = “+ kv.getTimestamp() + “\n”);</p>
<p>​         }</p>
<p>}</p>
<h2 id="4-13-hbase过滤器"><a href="#4-13-hbase过滤器" class="headerlink" title="4.13.   hbase过滤器"></a><a href="">4.13.   hbase</a>过滤器</h2><h3 id="4-13-1-FilterList"><a href="#4-13-1-FilterList" class="headerlink" title="4.13.1.           FilterList"></a><a href="">4.13.1.           FilterList</a></h3><p>FilterList 代表一个过滤器列表，可以添加多个过滤器进行查询，多个过滤器之间的关系有：</p>
<p>与关系（符合所有）：FilterList.Operator.MUST_PASS_ALL</p>
<p>或关系（符合任一）：FilterList.Operator.MUST_PASS_ONE</p>
<p>使用方法：</p>
<p>FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ONE);   </p>
<p>Scan s1 = new Scan();  </p>
<p> filterList.addFilter(new SingleColumnValueFilter(Bytes.toBytes(“f1”),  Bytes.toBytes(“c1”),  CompareOp.EQUAL,Bytes.toBytes(“v1”) )  );  </p>
<p>filterList.addFilter(new SingleColumnValueFilter(Bytes.toBytes(“f1”),  Bytes.toBytes(“c2”),  CompareOp.EQUAL,Bytes.toBytes(“v2”) )  );  </p>
<p> // 添加下面这一行后，则只返回指定的cell，同一行中的其他cell不返回  </p>
<p> s1.addColumn(Bytes.toBytes(“f1”), Bytes.toBytes(“c1”));  </p>
<p> s1.setFilter(filterList);  //设置filter</p>
<p> ResultScanner ResultScannerFilterList = table.getScanner(s1);  //返回结果列表</p>
<h3 id="4-13-2-过滤器的种类"><a href="#4-13-2-过滤器的种类" class="headerlink" title="4.13.2.           过滤器的种类"></a><a href="">4.13.2.           过滤器的种类</a></h3><p>过滤器的种类：</p>
<p>列值过滤器—SingleColumnValueFilter </p>
<p>​     过滤列植的相等、不等、范围等</p>
<p>列名前缀过滤器—ColumnPrefixFilter </p>
<p>​     过滤指定前缀的列名</p>
<p>多个列名前缀过滤器—MultipleColumnPrefixFilter</p>
<p>​      过滤多个指定前缀的列名</p>
<p>rowKey过滤器—RowFilter</p>
<p>​     通过正则，过滤rowKey值。</p>
<h3 id="4-13-3-列植过滤器—SingleColumnValueFilter"><a href="#4-13-3-列植过滤器—SingleColumnValueFilter" class="headerlink" title="4.13.3.           列植过滤器—SingleColumnValueFilter"></a><a href="">4.13.3.           列植过滤器—SingleColumnValueFilter</a></h3><p>SingleColumnValueFilter 列值判断</p>
<p>相等 (CompareOp.EQUAL ), </p>
<p>不等(CompareOp.NOT_EQUAL),</p>
<p>范围 (e.g., CompareOp.GREATER)…………</p>
<p>下面示例检查列值和字符串’values’ 相等…</p>
<p>SingleColumnValueFilter f = new  SingleColumnValueFilter(</p>
<p>​                            Bytes.toBytes(“cFamily”)                                                Bytes.toBytes(“column”),                      CompareFilter.CompareOp.EQUAL,</p>
<p>​       Bytes.toBytes(“values”));</p>
<p>s1.setFilter(f);</p>
<p>注意：如果过滤器过滤的列在数据表中有的行中不存在，那么这个过滤器对此行无法过滤。</p>
<h3 id="4-13-4-列名前缀过滤器—ColumnPrefixFilter"><a href="#4-13-4-列名前缀过滤器—ColumnPrefixFilter" class="headerlink" title="4.13.4.           列名前缀过滤器—ColumnPrefixFilter"></a><a href="">4.13.4.           列名前缀过滤器—ColumnPrefixFilter</a></h3><p>过滤器—ColumnPrefixFilter </p>
<p>ColumnPrefixFilter 用于指定列名前缀值相等</p>
<p>ColumnPrefixFilter f = newColumnPrefixFilter(Bytes.toBytes(“values”));</p>
<p>s1.setFilter(f);</p>
<h3 id="4-13-5-多个列值前缀过滤器—MultipleColumnPrefixFilter"><a href="#4-13-5-多个列值前缀过滤器—MultipleColumnPrefixFilter" class="headerlink" title="4.13.5.           多个列值前缀过滤器—MultipleColumnPrefixFilter"></a><a href="">4.13.5.           多个列值前缀过滤器—MultipleColumnPrefixFilter</a></h3><p>MultipleColumnPrefixFilter 和ColumnPrefixFilter 行为差不多，但可以指定多个前缀</p>
<p>byte[][] prefixes = new byte[][]{Bytes.toBytes(“value1”),Bytes.toBytes(“value2”)};</p>
<p>Filter f = newMultipleColumnPrefixFilter(prefixes);</p>
<p>s1.setFilter(f);</p>
<h3 id="4-13-6-rowKey过滤器—RowFilter"><a href="#4-13-6-rowKey过滤器—RowFilter" class="headerlink" title="4.13.6.           rowKey过滤器—RowFilter"></a><a href="">4.13.6.           rowKey</a>过滤器—RowFilter</h3><p>RowFilter 是rowkey过滤器</p>
<p>通常根据rowkey来指定范围时，使用scan扫描器的StartRow和StopRow方法比较好。</p>
<p>Filter f = newRowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(“^1234”));//匹配以1234开头的rowkey</p>
<p>s1.setFilter(f);</p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hbase </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[MapReduce运行原理详解]]></title>
      <url>/2018/01/17/MapReduce%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/</url>
      <content type="html"><![CDATA[<p>我们通过提交jar包，进行MapReduce处理，那么整个运行过程分为五个环节：</p>
<p>　　1、向client端提交MapReduce job.</p>
<p>　　2、随后yarn的ResourceManager进行资源的分配.</p>
<p>　　3、由NodeManager进行加载与监控containers.</p>
<p>　　4、通过applicationMaster与ResourceManager进行资源的申请及状态的交互，由NodeManagers进行MapReduce运行时job的管理.</p>
<p>　　5、通过hdfs进行job配置文件、jar包的各节点分发。</p>
<p>​       <img src="/img/mapreduce104.jpg" alt="mapreduce"></p>
<p><strong>Job 提交过程</strong></p>
<p>　　job的提交通过<strong>调用submit()方法</strong>创建一个<strong>JobSubmitter</strong>实例，并<strong>调用submitJobInternal()</strong>方法。整个job的运行过程如下：</p>
<p>　　1、向ResourceManager申请application ID，此ID为该MapReduce的jobId。</p>
<p>　　2、检查output的路径是否正确，是否已经被创建。</p>
<p>　　3、计算input的splits。</p>
<p>　　4、拷贝运行job 需要的jar包、配置文件以及计算input的split 到各个节点。</p>
<p>　　5、在ResourceManager中调用submitAppliction()方法,执行job</p>
<p><strong>Job 初始化过程</strong></p>
<p>　　1、当resourceManager收到了submitApplication()方法的调用通知后，scheduler开始分配container,随之ResouceManager发送applicationMaster进程，告知每个nodeManager管理器。</p>
<p>　　2、<strong>由applicationMaster决定</strong>如何运行tasks,如果job数据量比较小，applicationMaster便选择<strong>将tasks运行在一个JVM中</strong>。那么如何判别这个job是大是小呢？当一个job的<strong>mappers数量小于10个</strong>，<strong>只有一个reducer或者读取的文件大小要小于一个HDFS block时</strong>，（可通过修改配置项mapreduce.job.ubertask.maxmaps,mapreduce.job.ubertask.maxreduces以及mapreduce.job.ubertask.maxbytes 进行调整)</p>
<p>　　3、在运行tasks之前，applicationMaster将会<strong>调用setupJob()方法</strong>，随之创建output的输出路径(这就能够解释，不管你的mapreduce一开始是否报错，输出路径都会创建)</p>
<p><strong>Task 任务分配</strong></p>
<p>　　1、接下来applicationMaster向ResourceManager请求containers用于执行map与reduce的tasks（step 8),这里map task的优先级要高于reduce task，当所有的map tasks结束后，随之进行sort(这里是shuffle过程后面再说）,最后进行reduce task的开始。(这里有一点，当map tasks执行了百分之5%的时候，将会请求reduce，具体下面再总结)</p>
<p>　　2、运行tasks的是需要消耗内存与CPU资源的，<strong>默认情况下，map和reduce的task资源分配为1024MB与一个核</strong>，（可修改运行的最小与最大参数配置,mapreduce.map.memory.mb,mapreduce.reduce.memory.mb,mapreduce.map.cpu.</p>
<p>vcores,mapreduce.reduce.reduce.cpu.vcores.)</p>
<p><strong>Task 任务执行</strong></p>
<p>　　1、这时一个task已经被ResourceManager分配到一个container中，由applicationMaster告知nodemanager启动container，这个task将会被一个<strong>主函数为YarnChild</strong>的java application运行，但在运行task之前，<strong>首先定位task需要的jar包、配置文件以及加载在缓存中的文件</strong>。</p>
<p>　　2、YarnChild运行于一个专属的JVM中，所以<strong>任何一个map或reduce任务出现问题，都不会影响整个nodemanager的crash或者hang</strong>。</p>
<p>　　3、每个task都可以在相同的JVM task中完成，随之将完成的处理数据写入临时文件中。</p>
<p>Mapreduce数据流</p>
<p><strong>运行进度与状态更新</strong></p>
<p>　　1、MapReduce是一个较长运行时间的批处理过程，可以是一小时、几小时甚至几天，那么Job的运行状态监控就非常重要。每个job以及<strong>每个task都有一个包含job（running,successfully completed,failed）的状态</strong>，以及value的计数器，状态信息及描述信息（描述信息一般都是在代码中加的打印信息），那么，这些信息是如何与客户端进行通信的呢？</p>
<p>　　2、当一个task开始执行，它将会保持运行记录，记录task完成的比例，对于map的任务，将会记录其运行的百分比，对于reduce来说可能复杂点，但系统依旧会估计reduce的完成比例。当一个map或reduce任务执行时，<strong>子进程会持续每三秒钟与applicationMaster进行交互</strong>。</p>
<p><strong>Job 完成</strong></p>
<p> 　　最终，applicationMaster会收到一个job完成的通知，随后改变job的状态为successful。最终，applicationMaster与task containers被清空。</p>
<p><strong>Shuffle与Sort</strong></p>
<p>　　从map到reduce的过程，被称之为shuffle过程，MapReduce使到reduce的数据一定是经过key的排序的，那么shuffle是如何运作的呢？</p>
<p>　　当map任务将数据output时，<strong>不仅仅是将结果输出到磁盘，它是将其写入内存缓冲区域，并进行一些预分类</strong>。</p>
<p><img src="/img/suffle-sort.jpg" alt="shuffle"></p>
<p><strong>　1、The Map Side</strong></p>
<p>　　首先map任务的<strong>output过程是一个环状的内存缓冲区，缓冲区的大小默认为100MB</strong>（可通过修改配置项mpareduce.task.io.sort.mb进行修改），当写入内存的大小到达一定比例<strong>，默认为80%</strong>（可通过mapreduce.map.sort.spill.percent配置项修改）,便开始写入磁盘。</p>
<p>　　在写入磁盘之前，线程将会指定数据写入与reduce相应的patitions中，最终传送给reduce.在每个partition中<strong>，后台线程将会在内存中进行Key的排序</strong>，（<strong>如果代码中有combiner方法，则会在output时就进行sort排序</strong>，这里，如果只有少于3个写入磁盘的文件，combiner将会在outputfile前启动，如果只有一个或两个，那么将不会调用）</p>
<p>　 这里<strong>将map输出的结果进行压缩会大大减少磁盘IO与网络传输的开销</strong>（配置参数mapreduce.map .output.compress 设置为true,如果使用第三方压缩jar，可通过mapreduce.map.output.compress.codec进行设置)</p>
<p>　  随后这些paritions输出文件将会通过HTTP发送至reducers，传送的最大启动线程通过mapreduce.shuffle.max.threads进行配置。</p>
<p><strong>　　2、The Reduce Side</strong></p>
<p>　　首先上面每个节点的map都将结果写入了本地磁盘中，现在reduce需要将map的结果通过集群拉取过来，这里要注意的是，<strong>需要等到所有map任务结束后,reduce才会对map的结果进行拷贝</strong>，由于reduce函数有少数几个复制线程，以至于它<strong>可以同时拉取多个map的输出结果。默认的为5个线程</strong>（可通过修改配置mapreduce.reduce.shuffle.parallelcopies来修改其个数）</p>
<p>　　这里有个问题，那么reducers怎么知道从哪些机器拉取数据呢？ </p>
<p>　　当所有map的任务结束后，<strong>applicationMaster通过心跳机制（heartbeat mechanism)，由它知道mapping的输出结果与机器host</strong>,所以<strong>reducer会定时的通过一个线程访问applicationmaster请求map的输出结果</strong>。</p>
<p>　　Map的结果将会被拷贝到reduce task的JVM的内存中（内存大小可在mapreduce.reduce.shuffle.input.buffer.percent中设置）如果不够用，则会写入磁盘。当内存缓冲区的大小到达一定比例时（可通过mapreduce.reduce.shuffle.merge.percent设置)或map的输出结果文件过多时（可通过配置mapreduce.reduce.merge.inmen.threshold)，将会除法合并(merged)随之写入磁盘。</p>
<p>　　这时要注意，<strong>所有的map结果这时都是被压缩过的，需要先在内存中进行解压缩，以便后续合并它们</strong>。（合并最终文件的数量可通过mapreduce.task.io.sort.factor进行配置） 最终reduce进行运算进行输出。</p>
<p>参考文献:《Hadoop:The Definitive Guide, 4th Edition》 </p>
<p>本博文摘抄地址：<a href="https://www.cnblogs.com/yangsy0915/p/5559969.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangsy0915/p/5559969.html</a></p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> MapReduce </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hive安装之单机模式]]></title>
      <url>/2018/01/16/Hive%E5%AE%89%E8%A3%85%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F/</url>
      <content type="html"><![CDATA[<p>①下载Hive安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.bit.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz</span><br></pre></td></tr></table></figure>
<p>②安装mysql</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>1.下载mysql的repo源</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>2.安装mysql-community-release-el7-5.noarch.rpm包</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>安装这个包后，会获得两个mysql的yum repo源：</span><br><span class="line">/etc/yum.repos.d/mysql-community.repo，/etc/yum.repos.d/mysql-community-source.repo。</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>3.安装mysql</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo yum install mysql-server -y</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>根据提示安装就可以了,不过安装完成后没有密码,需要重置密码</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>4.重置mysql密码</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> mysql -u root</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>登录时有可能报这样的错：ERROR 2002 (HY000): Can‘t connect to local MySQL server through socket #‘/var/lib/mysql/mysql.sock‘ (2)，原因是/var/lib/mysql的访问权限问题。下面的命令把/var/lib/mysql的拥有#者改为当前用户：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo chown -R root:root /var/lib/mysql</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>重启mysql服务</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> service mysqld restart</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>接下来登录重置密码：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> mysql -u root  //直接回车进入mysql控制台</span><br><span class="line">mysql &gt; use mysql;</span><br><span class="line">mysql &gt; update user set password=password('123456') where user='root';</span><br><span class="line">mysql &gt; exit;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>再次重启mysql</span><br><span class="line"><span class="meta">$</span> service mysqld restart</span><br><span class="line"><span class="meta">$</span> mysql -u root -p </span><br><span class="line">Enter password: 此处输入密码</span><br></pre></td></tr></table></figure>
<p>③安装Hive</p>
<p>(1）解压Hive压缩包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.2-bin.tar.gz -C /usr/local</span><br><span class="line"></span><br><span class="line">cd /usr/local</span><br><span class="line"></span><br><span class="line">mv apache-hive-1.2.2-bin hive-1.2.2</span><br></pre></td></tr></table></figure>
<p>(2）修改配置文件</p>
<p>配置文件重命名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive-1.2.2</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line">cp hive-default.xml.template hive-site.xml</span><br><span class="line">cp hive-log4j2.properties.template hive-log4j2.properties</span><br><span class="line">cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties</span><br></pre></td></tr></table></figure>
<p>配置文件修改</p>
<p>hive-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HEAPSIZE=1024</span><br><span class="line">HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.2</span><br><span class="line">export HIVE_CONF_DIR=/usr/local/hive-1.2.2/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/usr/local/hive-1.2.2/lib</span><br></pre></td></tr></table></figure>
<p>(3)创建HDFS目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hive/warehouse</span><br><span class="line">hdfs dfs -mkdir -p /user/hive/tmp</span><br><span class="line">hdfs dfs -mkdir -p /user/hive/log</span><br><span class="line">hdfs dfs -chmod g+w /user/hive/warehouse</span><br><span class="line">hdfs dfs -chmod g+w /usr/hive/tmp</span><br><span class="line">hdfs dfs -chmod g+w /usr/hive/log</span><br></pre></td></tr></table></figure>
<p>(4)创建数据库和hive用户</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">假定你已经安装好 MySQL。下面创建一个 hive 数据库用来存储 Hive 元数据，且数据库访问的用户名和密码都为 hive。</span><br><span class="line"></span><br><span class="line">mysql&gt; CREATE DATABASE hive; </span><br><span class="line">mysql&gt; USE hive; </span><br><span class="line">mysql&gt; CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';</span><br><span class="line">mysql&gt; GRANT ALL ON hive.* TO 'hive'@'localhost' IDENTIFIED BY 'hive'; </span><br><span class="line">mysql&gt; GRANT ALL ON hive.* TO 'hive'@'%' IDENTIFIED BY 'hive'; </span><br><span class="line">mysql&gt; FLUSH PRIVILEGES; </span><br><span class="line">mysql&gt; quit;</span><br><span class="line">修改hive-site.xml</span><br><span class="line">需要在 hive-site.xml 文件中配置 MySQL 数据库连接信息。</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">运行Hive</span><br><span class="line">在命令行运行 hive 命令时必须保证以下两点：</span><br><span class="line"></span><br><span class="line">HDFS 已经启动。可以使用 start-dfs.sh 脚本来启动 HDFS。</span><br><span class="line">MySQL Java 连接器添加到 $HIVE_HOME/lib 目录下。我安装时使用的是 mysql-connector-java-5.1.39.jar。</span><br><span class="line">从 Hive 2.1 版本开始, 我们需要先运行 schematool 命令来执行初始化操作。</span><br><span class="line"></span><br><span class="line">schematool -dbType mysql -initSchema</span><br><span class="line"></span><br><span class="line">报错：缺少mysql jar包</span><br><span class="line">解决：将其（如mysql-connector-Java-5.1.15-bin.jar）拷贝到$HIVE_HOME/lib下即可。</span><br></pre></td></tr></table></figure>
<p>启动Hive时报以下错误:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span>java.lang.RuntimeException: java.lang.IllegalArgumentException:java.net.URISyntaxException: Relative path in absolute URI:$&#123;system:java.io.tmpdir%<span class="number">7</span>D/$%<span class="number">7</span>Bsystem:user.name%<span class="number">7</span>D</span><br><span class="line">        atorg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">444</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:<span class="number">672</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:<span class="number">616</span>)</span><br><span class="line">        atsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        atsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">57</span>)</span><br><span class="line">        atsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">        atjava.lang.reflect.Method.invoke(Method.java:<span class="number">606</span>)</span><br><span class="line">        atorg.apache.hadoop.util.RunJar.main(RunJar.java:<span class="number">160</span>)</span><br><span class="line">Caused by: java.lang.IllegalArgumentException:java.net.URISyntaxException: Relative path in absolute URI:$&#123;system:java.io.tmpdir%<span class="number">7</span>D/$%<span class="number">7</span>Bsystem:user.name%<span class="number">7</span>D</span><br><span class="line">        atorg.apache.hadoop.fs.Path.initialize(Path.java:<span class="number">148</span>)</span><br><span class="line">        atorg.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:<span class="number">126</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:<span class="number">487</span>)</span><br><span class="line">        atorg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">430</span>)</span><br><span class="line">        ... <span class="number">7</span>more</span><br></pre></td></tr></table></figure>
<p>解决方案：将 hive-site.xml 中的 ${system:java.io.tmpdir}  和  ${system:user.name} 分别替换成 /tmp 和 ${user.name}</p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mahout安装之单机模式]]></title>
      <url>/2018/01/16/Mahout%E5%AE%89%E8%A3%85%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F/</url>
      <content type="html"></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hbase </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hbase之单机模式安装]]></title>
      <url>/2018/01/16/Hbase%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
      <content type="html"><![CDATA[<p>①下载hbase安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.bit.edu.cn/apache/hbase/1.2.6/hbase-1.2.6-bin.tar.gz</span><br></pre></td></tr></table></figure>
<p>②解压安装包到指定目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/local/hbase</span><br><span class="line">tar -zxvf hbase-1.2.6-bin.tar.gz -C /usr/local/hbase</span><br></pre></td></tr></table></figure>
<p>③修改配置文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hbase/hbase-1.2.6/conf</span><br><span class="line"></span><br><span class="line">vi hbase-site.xml</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.19.128:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">vi hbase-env.sh</span><br><span class="line"># hbase-env.sh添加JAVA_HOME配置</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_131/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line">HBASE_HOME=/usr/local/hbase/hbase-1.2.6</span><br><span class="line">PATH=$HBASE_HOME/bin:$PATH</span><br><span class="line">export HBASE_HOME PATH</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>④启动Hbase</p>
<p>启动hadoop :       start-all.sh</p>
<p>启动zookeeper： zkServer.sh start</p>
<p>启动Hbase:          start-hbase.sh</p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hbase </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Java的native方法]]></title>
      <url>/2018/01/13/Java%E7%9A%84native%E6%96%B9%E6%B3%95/</url>
      <content type="html"></content>
      
        <categories>
            
            <category> Java </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Java虚拟机 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark2.0.0单机模式安装]]></title>
      <url>/2018/01/12/Spark2-0-0%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
      <content type="html"><![CDATA[<p>①解压已下载的spark压缩包</p>
<p>tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz -C /usr/local/spark/spark-2.0.0-bin-hadoop2.7</p>
<p>②配置文件修改</p>
<p>/conf spark-env.sh末尾加上 export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/hadoop-2.7.2/bin/hadoop classpath)</p>
<p>②配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME=/usr/local/spark/spark-2.0.0-bin-hadoop2.7</span><br><span class="line">PATH=$SPARK_HOME/bin:$PATH </span><br><span class="line">export SPARK_HOME PATH</span><br></pre></td></tr></table></figure>
<p>③启动spark   ./sbin/start-master.sh</p>
<p>命令：关闭spark : ./sbin/stop-master.sh</p>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Zookeeper安装之单机模式]]></title>
      <url>/2018/01/12/Zookeeper%E5%AE%89%E8%A3%85%E4%B9%8B%E5%8D%95%E6%9C%BA%E6%A8%A1%E5%BC%8F/</url>
      <content type="html"><![CDATA[<p>①下载zookeeper</p>
<p>wget <a href="http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz</a></p>
<p>②解压 zookeeper到指定目录</p>
<p>mkdir /usr/local/zookeeper</p>
<p>tar -zxvf zookeeper-3.3.6.tar.gz -C /usr/local/zookeeper/</p>
<p>③复制 zoo_sample.cfg 为zoo.cfg</p>
<p>cd /usr/local/zookeeper/zookeeper-3.3.6/conf/</p>
<p>cp zoo_sample.cfg zoo.cfg</p>
<p>④设置zookeeper环境变量</p>
<p>vi /etc/profile</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZOOKEEPER_HOME=/usr/<span class="built_in">local</span>/zookeeper/zookeeper-3.3.6</span><br><span class="line">PATH=<span class="variable">$ZOOKEEPER_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME PATH</span><br></pre></td></tr></table></figure>
<p>source /etc/profile</p>
<p>④启动zookeeper</p>
<p>zkServer.sh start  启动命令  |    zkServer.sh status 查看状态</p>
<p>⑤客户端连接</p>
<p>zkCli.sh -server 192.168.19.128:2181 </p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> zookeeper </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark之WorkCount]]></title>
      <url>/2018/01/11/Spark%E4%B9%8BWorkCount/</url>
      <content type="html"><![CDATA[<p>①使用IntelliJ IDEA新建Scala-maven项目</p>
<p>②Spark workcount程序编写</p>
<p> 第一步配置pom.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.spark.test<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inceptionYear</span>&gt;</span>2008<span class="tag">&lt;/<span class="name">inceptionYear</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-make:transitive<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-dependencyfile<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;project.build.directory&#125;/.scala_dependencies<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span>                   <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>guiguzi.spark.test.Test<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>第二步：编写workcount程序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> guiguzi.spark.test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      conf.setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      <span class="keyword">val</span> file = <span class="string">"hdfs://192.168.19.128:9000/user/hadoop/1.txt"</span></span><br><span class="line">      <span class="keyword">val</span> lines = sc.textFile(file)</span><br><span class="line">      <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      <span class="keyword">val</span> wordCount = words.countByValue()</span><br><span class="line">      println(wordCount)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>③把工程打成jar包 上传到服务器</p>
<p>双击下图的package，项目会生成相应的jar,将jar上传到服务器即可。</p>
<p><img src="/img/package01.png" alt="截图"></p>
<p>④启动spark 启动程序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">启动程序命令：spark-submit --class guiguzi.spark.test.Test spark-test-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<p>有类似输出表明程序执行成功：Map(19 -&gt; 1, 192 -&gt; 3, 128 -&gt; 1, 12 -&gt; 1)</p>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> WorkCount </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop2.7.1单机环境安装]]></title>
      <url>/2018/01/11/Hadoop2-7%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</url>
      <content type="html"><![CDATA[<p>下载一份Hadoop.tar.gz<br>解压到你想要的目录， 我这里放到/usr/local/hadoop下。<br>修改如下配置文件：</p>
<p>cd  /usr/local/hadoop/hadoop-2.7.2/etc/hadoop/</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">core-site.xml:</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.19.128:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">hdfs-site.xml:</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.support.append<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>ssh密钥：<br>$ ssh-keygen -t dsa -P ‘’ -f ~/.ssh/id_dsa<br>$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>配置conf下hadoop-env.sh:<br>export JAVA_HOME=/usr/java/jdk1.8.0_131 #配置java_home<br>格式化namenode：<br>bin/hadoop namenode -format</p>
<p>增加环境变量：vi /etc/profile</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib.tools.jar</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export JAVA_HOME CLASSPATH PATH</span><br><span class="line">HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.2</span><br><span class="line">PATH=$HADOOP_HOME/bin:$PATH</span><br><span class="line">export HADOOP_HOME PATH</span><br></pre></td></tr></table></figure>
<p>生效环境变量：source /etc/profile</p>
<p>开启服务：<br>bin/start-all.sh </p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark从外部读取数据之textFile]]></title>
      <url>/2018/01/11/Spark%E4%BB%8E%E5%A4%96%E9%83%A8%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E4%B9%8BtextFile/</url>
      <content type="html"><![CDATA[<p>textFile函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * Read a text file from HDFS, a local file system (available on all nodes), or any </span></span><br><span class="line"><span class="comment"> * Hadoop-supported file system URI, and return it as an RDD of Strings. </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(  </span><br><span class="line">    path: <span class="type">String</span>,  </span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;  </span><br><span class="line">    assertNotStopped()  </span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],  </span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析参数：</p>
<p>path: String 是一个URI，這个URI可以是HDFS、本地文件（全部的节点都可以），或者其他Hadoop支持的文件系统URI返回的是一个字符串类型的RDD，也就是是RDD的内部形式是Iterator[(String)]</p>
<p>minPartitions=  math.min(defaultParallelism, 2) 是指定数据的分区，如果不指定分区，当你的核数大于2的时候，不指定分区数那么就是 2</p>
<p>当你的数据大于128M时候，Spark是为每一个快（block）创建一个分片（Hadoop-2.X之后为128m一个block）</p>
<p>1、从当前目录读取一个文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"Current.txt"</span>  <span class="comment">//Current fold file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>从当前目录读取一个Current.txt的文件</p>
<p>2、从当前目录读取多个文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"Current1.txt，Current2.txt，"</span>  <span class="comment">//Current fold file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>从当前读取两个文件，分别是Cuttent1.txt和Current2.txt</p>
<p>3、从本地系统读取一个文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/README.md"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>4、从本地系统读取整个文件夹</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>从本地系统中读取licenses这个文件夹下的所有文件</p>
<p>這里特别注意的是，比如這个文件夹下有35个文件，上面分区数设置是2，那么整个RDD的分区数是35*2？</p>
<p>這是错误的，這个RDD的分区数不管你的partition数设置为多少时，只要license這个文件夹下的這个文件a.txt</p>
<p>(比如有a.txt)没有超过128m，那么a.txt就只有一个partition。那么就是说只要这35个文件其中没有一个超过</p>
<p>128m，那么分区数就是 35个</p>
<p>5、从本地系统读取多个文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/LICENSE-scala.txt,file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/LICENSE-spire.txt"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>从本地系统中读取file:///usr/local/spark/spark-1.6.0-bin-hadoop2.6/licenses/下的LICENSE-spire.txt和</p>
<p>LICENSE-scala.txt两个文件。上面分区设置是2，那个RDD的整个分区数是2*2</p>
<p>6、从本地系统读取多个文件夹下的文件（把如下文件全部读取进来）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"/usr/local/spark/spark-1.6.0-bin-hadoop2.6/data/*/*"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>采用通配符的形式来代替文件，来对数据文件夹进行整体读取。但是后面设置的分区数2也是可以去除的。因为一个文件没有达到128m，所以上面的一个文件一个partition，一共是20个。</p>
<p>7采用通配符，来读取多个文件名类似的文件</p>
<p>比如读取如下文件的people1.txt和people2.txt,但google.txt不读取</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">2</span>)&#123;  </span><br><span class="line">      <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">s"/root/application/temp/people<span class="subst">$i</span>*"</span>,<span class="number">2</span>)  </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>8、采用通配符读取相同后缀的文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"/usr/local/spark/spark-1.6.0-bin-hadoop2.6/data/*/*.txt"</span>  <span class="comment">//local file  </span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>9、从HDFS读取一个文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"hdfs://master:9000/examples/examples/src/main/resources/people.txt"</span>  </span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>从HDFS中读取文件的形式和本地上一样，只是前面的路径要表明是HDFS中的</p>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Centos安装Hadoop3.0]]></title>
      <url>/2018/01/06/centos-hadoop/centos-hadoop/</url>
      <content type="html"><![CDATA[<p>1、创建目录 解压文件到指定目录</p>
<p>mkdir /usr/hadoop</p>
<p>tar -zxvf hadoop.tar.gz -C /usr/hadoop</p>
<p>2、设置hadoop</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">一共需要配置主要的6个文件</span><br><span class="line">    hadoop-3.0.0/etc/hadoop/hadoop-env.sh </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/yarn-env.sh </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/core-site.xml </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/hdfs-site.xml </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/mapred-site.xml </span><br><span class="line">    hadoop-3.0.0/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure>
<p>1）配置hadoop-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=$&#123;JAVA_HOME&#125;</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line"></span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_SECURE_USER=root</span><br><span class="line">export HDFS_DATANODE_SECURE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure>
<p>​     2）配置yarn-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>
<p>3）配置core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.19.128:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>HDFS的URI，文件系统://namenode标识:端口号<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode上本地的hadoop临时文件夹<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>4），配置hdfs-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">!—hdfs-site.xml--</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode上存储hdfs名字空间元数据 <span class="tag">&lt;/<span class="name">description</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode上数据块的物理存储位置<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>副本个数，配置默认是3,应小于datanode机器数量<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>5）配置mapred-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>6）配置yarn-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs:192.168.19.128:8099<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">description</span>&gt;</span>这个地址是mr管理界面的<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>3、启动及测试<br>①格式化namenode,</p>
<p>cd /usr/hadoop/hadoop-3.0.0    ./bin/hdfs namenode -format</p>
<p>②启动</p>
<p>cd /usr/hadoop/hadoop-3.0.0 ./sbin/start-all.sh</p>
<p>③访问 <a href="http://192.168.19.128:9870" target="_blank" rel="noopener">http://192.168.19.128:9870</a></p>
<p>4、设置环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">HADOOP_HOME=/usr/hadoop/hadoop-3.0.0</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$PATH</span><br><span class="line">export HADOOP_HOME PATH</span><br></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Centos7安装java]]></title>
      <url>/2018/01/06/centos-java/index/</url>
      <content type="html"><![CDATA[<p>1、安装yum : yum -y update</p>
<p>2、安装上传插件 ： yum install lrzsz</p>
<p>3、上传已下载的 jdk tar包 jdk-8u131-linux-x64.tar.gz</p>
<p>​       rz -y 选择tar包</p>
<p>4、解压 ：tar -zxvf  jdk-8u131-linux-x64.tar.gz -C /usr/java</p>
<p>5、设置环境变量：</p>
<p>​     vi /etc/profile</p>
<p>​      ①在末尾添加在末尾行添加</p>
<p>​      ②在末尾行添加</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">set</span> java environment</span></span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib.tools.jar</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure>
<p>​      ③source /etc/profile</p>
<p>​      ④java -version 查看JDK版本信息，如果显示以下信息证明安装成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version "1.8.0_131"</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_131-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)</span><br></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[terminal capability ＂cm＂ required 解决办法]]></title>
      <url>/2018/01/05/error/index/</url>
      <content type="html"><![CDATA[<p>E437: terminal capability “cm” required</p>
<p>这个错误一般是环境变量TERM没有配置或者配置错误所致。</p>
<p>解决办法：</p>
<p>执行export TERM=xterm；</p>
<p>或者将export TERM=xterm 添加至profile文件中即可。</p>
<p>原文地址：<a href="http://blog.csdn.net/budory/article/details/47256995" target="_blank" rel="noopener">http://blog.csdn.net/budory/article/details/47256995</a></p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Java链式编程]]></title>
      <url>/2018/01/05/java-chain/index/</url>
      <content type="html"><![CDATA[<p>链式编程思想：是将多个操作（多行代码）通过点号(.)链接在一起成为一句代码,使代码可读性好</p>
<p>链式编程特点：方法的返回值是block,block必须有返回值（本身对象），block参数（需要操作的值）</p>
<p>简单示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">	<span class="keyword">private</span> String id ;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">private</span> String username;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> id;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> User <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.id = id;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getUsername</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> username;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> User <span class="title">setUsername</span><span class="params">(String username)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.username = username;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">User user =<span class="keyword">new</span> User();</span><br><span class="line">user.setId(<span class="string">"1"</span>)</span><br><span class="line">    .setUsername(<span class="string">"张三"</span>);</span><br></pre></td></tr></table></figure>
<p>lombok中的@Accessors(fluent=true)也能实现链式编程</p>
<p>简单示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lombok.Data;</span><br><span class="line"><span class="keyword">import</span> lombok.experimental.Accessors;</span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Accessors</span>(fluent=<span class="keyword">true</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String id ;</span><br><span class="line">    <span class="keyword">private</span> String username;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User user = <span class="keyword">new</span> User();</span><br><span class="line">user.id(<span class="string">"001"</span>).username(<span class="string">"张三"</span>);</span><br></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Java </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Java后端 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[mapreduce]]></title>
      <url>/2017/12/31/mapreduce/index/</url>
      <content type="html"><![CDATA[<p>mapreduce概述：</p>
<p>1、分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题 </p>
<p>2、阶段：Map和reduce</p>
<p>3、特点：高容错性、高扩展、编程简单、适合大数据离线批量处理</p>
<p>流程分析：</p>
<p>词频统计：</p>
<p>​                 代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.test.mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Guiguzi</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountMapReduce</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * KEYIN</span></span><br><span class="line"><span class="comment">     * VALUEIN</span></span><br><span class="line"><span class="comment">     * KEYOUT</span></span><br><span class="line"><span class="comment">     * VALUEOUT</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * map方法</span></span><br><span class="line"><span class="comment">         * map阶段的key-value对的格式是由输入的格式所决定的，如果是默认的TextInputFormat，</span></span><br><span class="line"><span class="comment">         * 则每行作为一个记录进程处理，其中key为此行的开头相对于文件的起始位置，value就是此行的字符文本</span></span><br><span class="line"><span class="comment">         * map阶段的输出的key-value对的格式必须同reduce阶段的输入key-value对的格式相对应</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String line = value.toString();</span><br><span class="line">            String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word:words) &#123;</span><br><span class="line">                context.write(<span class="keyword">new</span> Text(word),<span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CountReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> values</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable i : values)&#123;</span><br><span class="line">                        count +=i.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key,<span class="keyword">new</span> IntWritable(count));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 创建配置对象</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 创建job对象</span></span><br><span class="line">    Job job = Job.getInstance(conf, <span class="string">"wordcount"</span>);</span><br><span class="line">    <span class="comment">// 设置运行job的主类</span></span><br><span class="line">    job.setJarByClass(CountMapReduce.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置mapper类</span></span><br><span class="line">    job.setMapperClass(CountMapper.class);</span><br><span class="line">    <span class="comment">// 设置reducer类</span></span><br><span class="line">    job.setReducerClass(CountReduce.class);</span><br><span class="line">    <span class="comment">// 设置map输出的key value</span></span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    <span class="comment">// 设置reducer输出的key value类型</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    <span class="comment">// 设置输入输入的路径</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 提交job</span></span><br><span class="line">    <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">if</span>(!b) &#123;</span><br><span class="line">        System.err.println(<span class="string">"This task has failed!!!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>①创建 /user/hadoop目录</p>
<p>bin/hdfs dfs -mkdir -p /user/hadoop</p>
<p>②将文件复制到 /user/hadoop</p>
<p>hdfs dfs -put  1.txt  /user/hadoop</p>
<p>③启动程序</p>
<p>hadoop jar test.jar cn.test.mr.CountMapReduce  /user/hadoop/1.txt  out</p>
<p>④查看输出</p>
<p>hdfs dfs -cat out/*</p>
<p>具体命令参考 <a href="http://blog.csdn.net/wang_zhenwei/article/details/47444335" target="_blank" rel="noopener">http://blog.csdn.net/wang_zhenwei/article/details/47444335</a></p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> mapreduce </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[lombok]]></title>
      <url>/2017/12/31/lombok/index/</url>
      <content type="html"><![CDATA[<p><strong>lombok</strong> 提供了简单的注解的形式来帮助我们简化消除一些必须有但显得很臃肿的 java 代码</p>
<p>springboot集成lombok</p>
<p>pom文件添加以下依赖</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.projectlombok&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;lombok&lt;/artifactId&gt;</span><br><span class="line">	&lt;optional&gt;true&lt;/optional&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>lombok 注解：<br>​    lombok 提供的注解不多，可以参考官方视频的讲解和官方文档。<br>​    Lombok 注解在线帮助文档：<a href="http://projectlombok.org/features/index.html" target="_blank" rel="noopener">http://projectlombok.org/features/index.</a>    下面介绍几个我常用的 lombok 注解：<br>​        @Data   ：注解在类上；提供类所有属性的 getting 和 setting 方法，此外还提     供了equals、canEqual、hashCode、toString 方法<br>​        @Setter：注解在属性上；为属性提供 setting 方法<br>​        @Getter：注解在属性上；为属性提供 getting 方法<br>​        @Log4j ：注解在类上；为类提供一个 属性名为log 的 log4j 日志对象<br>​        @NoArgsConstructor：注解在类上；为类提供一个无参的构造方法<br>​        @AllArgsConstructor：注解在类上；为类提供一个全参的构造方法</p>
<p>示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Log</span>4j</span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String id;</span><br><span class="line"><span class="keyword">private</span> String name;</span><br><span class="line"><span class="keyword">private</span> String identity;  </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Java </category>
            
        </categories>
        
        
        <tags>
            
            <tag> lombok </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ThreadPoolTaskExecutor]]></title>
      <url>/2017/12/29/spring-tpe/index/</url>
      <content type="html"><![CDATA[<p>ThreadPoolTaskExecutor是一个spring的线程池技术，它是使用jdk中的java.util.concurrent.ThreadPoolExecutor进行实现。</p>
<p>spring配置ThreadPoolTaskExecutor：</p>
<p>xml配置方式：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"taskExecutor"</span> </span></span><br><span class="line"><span class="tag">   <span class="attr">class</span>=<span class="string">"org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor"</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"corePoolSize"</span> <span class="attr">value</span>=<span class="string">"20"</span> /&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"keepAliveSeconds"</span> <span class="attr">value</span>=<span class="string">"200"</span> /&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"maxPoolSize"</span> <span class="attr">value</span>=<span class="string">"50"</span> /&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"queueCapacity"</span> <span class="attr">value</span>=<span class="string">"50"</span> /&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注解配置方式：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaskExecutor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ThreadPoolTaskExecutor <span class="title">getExecutor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ThreadPoolTaskExecutor executor = <span class="keyword">new</span> ThreadPoolTaskExecutor();</span><br><span class="line">        executor.setCorePoolSize(<span class="number">20</span>);</span><br><span class="line">        executor.setKeepAliveSeconds(<span class="number">200</span>);</span><br><span class="line">        executor.setMaxPoolSize(<span class="number">50</span>);</span><br><span class="line">        executor.setQueueCapacity(<span class="number">50</span>);</span><br><span class="line">        <span class="keyword">return</span> executor;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Spring </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Java后端 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kafka]]></title>
      <url>/2017/12/29/kafka/index/</url>
      <content type="html"><![CDATA[<h1 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h1><p>Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消费。</p>
<h2 id="Spring-Boot-集成-Kafka"><a href="#Spring-Boot-集成-Kafka" class="headerlink" title="Spring Boot 集成 Kafka"></a>Spring Boot 集成 Kafka</h2><p>参考文档</p>
<p><a href="https://kafka.apache.org/" target="_blank" rel="noopener">https://kafka.apache.org</a></p>
<p><a href="https://projects.spring.io/spring-kafka/" target="_blank" rel="noopener">https://projects.spring.io/spring-kafka/</a></p>
<p><a href="http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-messaging.html#boot-features-kafka" target="_blank" rel="noopener">http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-messaging.html#boot-features-kafka</a></p>
<p><a href="http://docs.spring.io/spring-kafka/docs/1.2.2.RELEASE/reference/htmlsingle/" target="_blank" rel="noopener">http://docs.spring.io/spring-kafka/docs/1.2.2.RELEASE/reference/htmlsingle/</a></p>
<p>pom添加以下依赖</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;</span><br><span class="line">	&lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.projectlombok&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;lombok&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>application.properties </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># kafka</span><br><span class="line">spring.kafka.bootstrap-servers=localhost:<span class="number">9092</span></span><br><span class="line">spring.kafka.consumer.group-id=Guiguzi-group1</span><br><span class="line">spring.kafka.consumer.auto-offset-reset=earliest</span><br></pre></td></tr></table></figure>
<p>然后在 Spring Boot 中就可以使用 <code>KafkaTemplate</code> 发送消息，使用 <code>@KafkaListener</code> 消费指定主题的消息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.boot;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.KafkaListener;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.ResponseBody;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; template;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/send"</span>)</span><br><span class="line">    <span class="meta">@ResponseBody</span></span><br><span class="line">    <span class="function">String <span class="title">send</span><span class="params">(String topic, String key, String data)</span> </span>&#123;</span><br><span class="line">        template.send(topic, key, data);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"success"</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"t1"</span>, topics = <span class="string">"t1"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listenT1</span><span class="params">(ConsumerRecord&lt;?, ?&gt; cr)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        log.info(<span class="string">"&#123;&#125; - &#123;&#125; : &#123;&#125;"</span>, cr.topic(), cr.key(), cr.value());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(id = <span class="string">"t2"</span>, topics = <span class="string">"t2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listenT2</span><span class="params">(ConsumerRecord&lt;?, ?&gt; cr)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        log.info(<span class="string">"&#123;&#125; - &#123;&#125; : &#123;&#125;"</span>, cr.topic(), cr.key(), cr.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过<code>http://localhost:8080/send?topic=t2&amp;key=test1&amp;data=hello</code>方法可以产生指定主题的消息。或者可以通过前面使用命令行方式或者API方式创建消息。</p>
]]></content>
      
        <categories>
            
            <category> Hadoop </category>
            
        </categories>
        
        
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
